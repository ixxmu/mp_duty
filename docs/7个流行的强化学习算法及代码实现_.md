---
title: "7ä¸ªæµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åŠä»£ç å®ç°ï¼"
date: 2023-02-02T10:33:16Z
draft: ["false"]
tags: [
  "fetched",
  "æ³•çº³æ–¯ç‰¹"
]
categories: ["Acdemic"]
---
7ä¸ªæµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åŠä»£ç å®ç°ï¼ by æ³•çº³æ–¯ç‰¹
------
<div><p data-mpa-powered-by="yiban.io"><img data-copyright="0" data-ratio="0.16245006657789615" data-src="https://mmbiz.qpic.cn/mmbiz_gif/y0SBuxfLhaly8za5kQolL29ibZEM13gCSgBzGEYrqmlAjARVibQEMlGVQLmnNBUSamVczvRPz9sSRmiatSPsYu02g/640?wx_fmt=gif" data-type="gif" data-w="751" src="https://mmbiz.qpic.cn/mmbiz_gif/y0SBuxfLhaly8za5kQolL29ibZEM13gCSgBzGEYrqmlAjARVibQEMlGVQLmnNBUSamVczvRPz9sSRmiatSPsYu02g/640?wx_fmt=gif"><br></p><section><br></section><p><span>ä½œè€…<span>ä¸¨</span>Siddhartha Pramanik </span></p><p><span>æ¥æºä¸¨Deephub Imba</span></p><p><span md-inline="plain"></span></p><p cid="n2" mdtype="paragraph"><span md-inline="plain">å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°Fï½</span></p><p cid="n2" mdtype="paragraph"><span md-inline="plain">ç›®å‰æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åŒ…æ‹¬ Q-learningã€SARSAã€DDPGã€A2Cã€PPOã€DQN å’Œ TRPOã€‚è¿™äº›ç®—æ³•å·²è¢«ç”¨äºåœ¨æ¸¸æˆã€æœºå™¨äººå’Œå†³ç­–åˆ¶å®šç­‰å„ç§åº”ç”¨ä¸­ï¼Œå¹¶ä¸”è¿™äº›æµè¡Œçš„ç®—æ³•è¿˜åœ¨ä¸æ–­å‘å±•å’Œæ”¹è¿›ï¼Œæœ¬æ–‡æˆ‘ä»¬å°†å¯¹å…¶åšä¸€ä¸ªç®€å•çš„ä»‹ç»ã€‚</span></p><p><img data-ratio="0.5227272727272727" data-s="300,640" data-type="png" data-w="1100" data-src="https://mmbiz.qpic.cn/mmbiz_png/6wQyVOrkRNLia6E8LzSQWYVwFtoJzeA6jIL9rTkC6WBbWADtjrKgHCc1w8bQ9icZiaTC3kw4sdsgia3MaiaCj3LjaZA/640?wx_fmt=png" src="https://mmbiz.qpic.cn/mmbiz_png/6wQyVOrkRNLia6E8LzSQWYVwFtoJzeA6jIL9rTkC6WBbWADtjrKgHCc1w8bQ9icZiaTC3kw4sdsgia3MaiaCj3LjaZA/640?wx_fmt=png"></p><h2 cid="n7" mdtype="heading"><span md-inline="plain">1ã€Q-learning</span></h2><p cid="n4" mdtype="paragraph"><span md-inline="plain">Q-learningï¼šQ-learning æ˜¯ä¸€ç§æ— æ¨¡å‹ã€éç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®ƒä½¿ç”¨ Bellman æ–¹ç¨‹ä¼°è®¡æœ€ä½³åŠ¨ä½œå€¼å‡½æ•°ï¼Œè¯¥æ–¹ç¨‹è¿­ä»£åœ°æ›´æ–°ç»™å®šçŠ¶æ€åŠ¨ä½œå¯¹çš„ä¼°è®¡å€¼ã€‚Q-learning ä»¥å…¶ç®€å•æ€§å’Œå¤„ç†å¤§å‹è¿ç»­çŠ¶æ€ç©ºé—´çš„èƒ½åŠ›è€Œé—»åã€‚</span></p><p cid="n9" mdtype="paragraph"><span md-inline="plain">ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ Python å®ç° Q-learning çš„ç®€å•ç¤ºä¾‹ï¼š</span></p><section><pre><code><span>import</span>Â numpyÂ <span>as</span>Â np<br><br><span>#Â DefineÂ theÂ Q-tableÂ andÂ theÂ learningÂ rate</span><br>QÂ =Â np.zeros((state_space_size,Â action_space_size))<br>alphaÂ =Â <span>0.1</span><br><br><span>#Â DefineÂ theÂ explorationÂ rateÂ andÂ discountÂ factor</span><br>epsilonÂ =Â <span>0.1</span><br>gammaÂ =Â <span>0.99</span><br><br><span>for</span>Â episodeÂ <span>in</span>Â range(num_episodes):<br>Â Â Â Â current_stateÂ =Â initial_state<br>Â Â Â Â <span>while</span>Â <span>not</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â ChooseÂ anÂ actionÂ usingÂ anÂ epsilon-greedyÂ policy</span><br>Â Â Â Â Â Â Â Â <span>if</span>Â np.random.uniform(<span>0</span>,Â <span>1</span>)Â &lt;Â epsilon:<br>Â Â Â Â Â Â Â Â Â Â Â Â actionÂ =Â np.random.randint(<span>0</span>,Â action_space_size)<br>Â Â Â Â Â Â Â Â <span>else</span>:<br>Â Â Â Â Â Â Â Â Â Â Â Â actionÂ =Â np.argmax(Q[current_state])<br><br>Â Â Â Â Â Â Â Â <span>#Â TakeÂ theÂ actionÂ andÂ observeÂ theÂ nextÂ stateÂ andÂ reward</span><br>Â Â Â Â Â Â Â Â next_state,Â reward,Â doneÂ =Â take_action(current_state,Â action)<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ Q-tableÂ usingÂ theÂ BellmanÂ equation</span><br>Â Â Â Â Â Â Â Â Q[current_state,Â action]Â =Â Q[current_state,Â action]Â +Â alphaÂ *Â (<br>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â rewardÂ +Â gammaÂ *Â np.max(Q[next_state])Â -Â Q[current_state,Â action])<br><br>Â Â Â Â Â Â Â Â current_stateÂ =Â next_state<br></code></pre></section><p cid="n9" mdtype="paragraph"><span>ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œstate_space_size å’Œ actio</span><span>n_space_siz</span><span>e åˆ†åˆ«æ˜¯ç¯å¢ƒä¸­çš„çŠ¶æ€æ•°å’ŒåŠ¨ä½œæ•°ã€‚</span><span>num_episodes æ˜¯è¦ä¸ºè¿è¡Œç®—æ³•çš„è½®æ¬¡æ•°ã€‚</span><span>initial_state æ˜¯ç¯å¢ƒçš„èµ·å§‹çŠ¶æ€ã€‚</span><span>take_action(current_state, action) æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†å½“å‰çŠ¶æ€å’Œä¸€ä¸ªåŠ¨ä½œä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±å’Œä¸€ä¸ªæŒ‡ç¤ºè½®æ¬¡æ˜¯å¦å®Œæˆçš„å¸ƒå°”å€¼ã€‚</span></p><p cid="n43" mdtype="paragraph"><span md-inline="plain">åœ¨ while å¾ªç¯ä¸­ï¼Œä½¿ç”¨ epsilon-greedy ç­–ç•¥æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚ä½¿ç”¨æ¦‚ç‡ epsiloné€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œï¼Œä½¿ç”¨æ¦‚ç‡ 1-epsiloné€‰æ‹©å¯¹å½“å‰çŠ¶æ€å…·æœ‰æœ€é«˜ Q å€¼çš„åŠ¨ä½œã€‚</span></p><p cid="n44" mdtype="paragraph"><span md-inline="plain">é‡‡å–è¡ŒåŠ¨åï¼Œè§‚å¯Ÿä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ï¼Œä½¿ç”¨Bellmanæ–¹ç¨‹æ›´æ–°qã€‚å¹¶å°†å½“å‰çŠ¶æ€æ›´æ–°ä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚è¿™åªæ˜¯ Q-learning çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œå¹¶æœªè€ƒè™‘ Q-table çš„åˆå§‹åŒ–å’Œè¦è§£å†³çš„é—®é¢˜çš„å…·ä½“ç»†èŠ‚ã€‚</span></p><h2 cid="n15" mdtype="heading"><span md-inline="plain">2ã€SARSA</span></h2><p cid="n17" mdtype="paragraph"><span md-inline="plain"> SARSAï¼šSARSA æ˜¯ä¸€ç§æ— æ¨¡å‹ã€åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®ƒä¹Ÿä½¿ç”¨Bellmanæ–¹ç¨‹æ¥ä¼°è®¡åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œä½†å®ƒæ˜¯åŸºäºä¸‹ä¸€ä¸ªåŠ¨ä½œçš„æœŸæœ›å€¼ï¼Œè€Œä¸æ˜¯åƒ Q-learning ä¸­çš„æœ€ä¼˜åŠ¨ä½œã€‚SARSA ä»¥å…¶å¤„ç†éšæœºåŠ¨åŠ›å­¦é—®é¢˜çš„èƒ½åŠ›è€Œé—»åã€‚</span></p><section><pre><code><span>import</span>Â numpyÂ <span>as</span>Â np<br><br><span>#Â DefineÂ theÂ Q-tableÂ andÂ theÂ learningÂ rate</span><br>QÂ =Â np.zeros((state_space_size,Â action_space_size))<br>alphaÂ =Â <span>0.1</span><br><br><span>#Â DefineÂ theÂ explorationÂ rateÂ andÂ discountÂ factor</span><br>epsilonÂ =Â <span>0.1</span><br>gammaÂ =Â <span>0.99</span><br><br><span>for</span>Â episodeÂ <span>in</span>Â range(num_episodes):<br>Â Â Â Â current_stateÂ =Â initial_state<br>Â Â Â Â actionÂ =Â epsilon_greedy_policy(epsilon,Â Q,Â current_state)<br>Â Â Â Â <span>while</span>Â <span>not</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â TakeÂ theÂ actionÂ andÂ observeÂ theÂ nextÂ stateÂ andÂ reward</span><br>Â Â Â Â Â Â Â Â next_state,Â reward,Â doneÂ =Â take_action(current_state,Â action)<br>Â Â Â Â Â Â Â Â <span>#Â ChooseÂ nextÂ actionÂ usingÂ epsilon-greedyÂ policy</span><br>Â Â Â Â Â Â Â Â next_actionÂ =Â epsilon_greedy_policy(epsilon,Â Q,Â next_state)<br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ Q-tableÂ usingÂ theÂ BellmanÂ equation</span><br>Â Â Â Â Â Â Â Â Q[current_state,Â action]Â =Â Q[current_state,Â action]Â +Â alphaÂ *Â (<br>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â rewardÂ +Â gammaÂ *Â Q[next_state,Â next_action]Â -Â Q[current_state,Â action])<br>Â Â Â Â Â Â Â Â current_stateÂ =Â next_state<br>Â Â Â Â Â Â Â Â actionÂ =Â next_action<span></span></code></pre></section><p cid="n17" mdtype="paragraph"><span>state_space_size</span><span>å’Œaction_space_sizeåˆ†åˆ«æ˜¯ç¯å¢ƒä¸­çš„çŠ¶æ€å’Œæ“ä½œçš„æ•°é‡ã€‚</span><span>num_episodesæ˜¯æ‚¨æƒ³è¦è¿è¡ŒSARSAç®—æ³•çš„è½®æ¬¡æ•°ã€‚</span><span>Initial_stateæ˜¯ç¯å¢ƒçš„åˆå§‹çŠ¶æ€ã€‚</span><span>take_action(current_state, action)æ˜¯ä¸€ä¸ªå°†å½“å‰çŠ¶æ€å’Œä½œä¸ºæ“ä½œè¾“å…¥çš„å‡½æ•°ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±å’Œä¸€ä¸ªæŒ‡ç¤ºæƒ…èŠ‚æ˜¯å¦å®Œæˆçš„å¸ƒå°”å€¼ã€‚</span><br></p><p cid="n51" mdtype="paragraph"><span md-inline="plain">åœ¨whileå¾ªç¯ä¸­ï¼Œä½¿ç”¨åœ¨å•ç‹¬çš„å‡½æ•°epsilon_greedy_policy(epsilon, Q, current_state)ä¸­å®šä¹‰çš„epsilon-greedyç­–ç•¥æ¥æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æ“ä½œã€‚ä½¿ç”¨æ¦‚ç‡ epsiloné€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œï¼Œä½¿ç”¨æ¦‚ç‡ 1-epsilonå¯¹å½“å‰çŠ¶æ€å…·æœ‰æœ€é«˜ Q å€¼çš„åŠ¨ä½œã€‚</span></p><p cid="n52" mdtype="paragraph"><span md-inline="plain">ä¸Šé¢ä¸Q-learningç›¸åŒï¼Œä½†æ˜¯é‡‡å–äº†ä¸€ä¸ªè¡ŒåŠ¨åï¼Œåœ¨è§‚å¯Ÿä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±æ—¶å®ƒç„¶åä½¿ç”¨è´ªå¿ƒç­–ç•¥é€‰æ‹©ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ã€‚å¹¶ä½¿ç”¨Bellmanæ–¹ç¨‹æ›´æ–°qè¡¨ã€‚</span></p><h2 cid="n53" mdtype="heading"><span md-inline="plain">3ã€DDPG</span></h2><p cid="n23" mdtype="paragraph"><span md-inline="plain">DDPG æ˜¯ä¸€ç§ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´çš„æ— æ¨¡å‹ã€éç­–ç•¥ç®—æ³•ã€‚å®ƒæ˜¯ä¸€ç§actor-criticç®—æ³•ï¼Œå…¶ä¸­actorç½‘ç»œç”¨äºé€‰æ‹©åŠ¨ä½œï¼Œè€Œcriticç½‘ç»œç”¨äºè¯„ä¼°åŠ¨ä½œã€‚DDPG å¯¹äºæœºå™¨äººæ§åˆ¶å’Œå…¶ä»–è¿ç»­æ§åˆ¶ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ã€‚</span></p><section><pre><code><span>import</span>Â numpyÂ <span>as</span>Â np<br><span>from</span>Â keras.modelsÂ <span>import</span>Â Model,Â Sequential<br><span>from</span>Â keras.layersÂ <span>import</span>Â Dense,Â Input<br><span>from</span>Â keras.optimizersÂ <span>import</span>Â Adam<br><br><span>#Â DefineÂ theÂ actorÂ andÂ criticÂ models</span><br>actorÂ =Â Sequential()<br>actor.add(Dense(<span>32</span>,Â input_dim=state_space_size,Â activation=<span>'relu'</span>))<br>actor.add(Dense(<span>32</span>,Â activation=<span>'relu'</span>))<br>actor.add(Dense(action_space_size,Â activation=<span>'tanh'</span>))<br>actor.compile(loss=<span>'mse'</span>,Â optimizer=Adam(lr=<span>0.001</span>))<br><br>criticÂ =Â Sequential()<br>critic.add(Dense(<span>32</span>,Â input_dim=state_space_size,Â activation=<span>'relu'</span>))<br>critic.add(Dense(<span>32</span>,Â activation=<span>'relu'</span>))<br>critic.add(Dense(<span>1</span>,Â activation=<span>'linear'</span>))<br>critic.compile(loss=<span>'mse'</span>,Â optimizer=Adam(lr=<span>0.001</span>))<br><br><span>#Â DefineÂ theÂ replayÂ buffer</span><br>replay_bufferÂ =Â []<br><br><span>#Â DefineÂ theÂ explorationÂ noise</span><br>exploration_noiseÂ =Â OrnsteinUhlenbeckProcess(size=action_space_size,Â theta=<span>0.15</span>,Â mu=<span>0</span>,Â sigma=<span>0.2</span>)<br><br><span>for</span>Â episodeÂ <span>in</span>Â range(num_episodes):<br>Â Â Â Â current_stateÂ =Â initial_state<br>Â Â Â Â <span>while</span>Â <span>not</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â SelectÂ anÂ actionÂ usingÂ theÂ actorÂ modelÂ andÂ addÂ explorationÂ noise</span><br>Â Â Â Â Â Â Â Â actionÂ =Â actor.predict(current_state)[<span>0</span>]Â +Â exploration_noise.sample()<br>Â Â Â Â Â Â Â Â actionÂ =Â np.clip(action,Â <span>-1</span>,Â <span>1</span>)<br><br>Â Â Â Â Â Â Â Â <span>#Â TakeÂ theÂ actionÂ andÂ observeÂ theÂ nextÂ stateÂ andÂ reward</span><br>Â Â Â Â Â Â Â Â next_state,Â reward,Â doneÂ =Â take_action(current_state,Â action)<br><br>Â Â Â Â Â Â Â Â <span>#Â AddÂ theÂ experienceÂ toÂ theÂ replayÂ buffer</span><br>Â Â Â Â Â Â Â Â replay_buffer.append((current_state,Â action,Â reward,Â next_state,Â done))<br><br>Â Â Â Â Â Â Â Â <span>#Â SampleÂ aÂ batchÂ ofÂ experiencesÂ fromÂ theÂ replayÂ buffer</span><br>Â Â Â Â Â Â Â Â batchÂ =Â sample(replay_buffer,Â batch_size)<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ criticÂ model</span><br>Â Â Â Â Â Â Â Â statesÂ =Â np.array([x[<span>0</span>]Â <span>for</span>Â xÂ <span>in</span>Â batch])<br>Â Â Â Â Â Â Â Â actionsÂ =Â np.array([x[<span>1</span>]Â <span>for</span>Â xÂ <span>in</span>Â batch])<br>Â Â Â Â Â Â Â Â rewardsÂ =Â np.array([x[<span>2</span>]Â <span>for</span>Â xÂ <span>in</span>Â batch])<br>Â Â Â Â Â Â Â Â next_statesÂ =Â np.array([x[<span>3</span>]Â <span>for</span>Â xÂ <span>in</span>Â batch])<br><br>Â Â Â Â Â Â Â Â target_q_valuesÂ =Â rewardsÂ +Â gammaÂ *Â critic.predict(next_states)<br>Â Â Â Â Â Â Â Â critic.train_on_batch(states,Â target_q_values)<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ actorÂ model</span><br>Â Â Â Â Â Â Â Â action_gradientsÂ =Â np.array(critic.get_gradients(states,Â actions))<br>Â Â Â Â Â Â Â Â actor.train_on_batch(states,Â action_gradients)<br><br>Â Â Â Â Â Â Â Â current_stateÂ =Â next_state<br></code></pre></section><p cid="n23" mdtype="paragraph"><span>åœ¨æœ¬ä¾‹ä¸­ï¼Œstate_space_sizeå’Œaction_space_sizeåˆ†åˆ«æ˜¯ç¯å¢ƒä¸­çš„çŠ¶æ€å’Œæ“ä½œçš„æ•°é‡ã€‚</span><span>num_episodesæ˜¯è½®æ¬¡æ•°ã€‚</span><span>Initial_stateæ˜¯ç¯å¢ƒçš„åˆå§‹çŠ¶æ€ã€‚</span><span>Take_action (current_state, action)æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—å½“å‰çŠ¶æ€å’Œæ“ä½œä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªæ“ä½œã€‚</span></p><h2 cid="n29" mdtype="heading"><span md-inline="plain">4ã€A2C</span></h2><p cid="n31" mdtype="paragraph"><span md-inline="plain">A2Cï¼ˆAdvantage Actor-Criticï¼‰æ˜¯ä¸€ç§æœ‰ç­–ç•¥çš„actor-criticç®—æ³•ï¼Œå®ƒä½¿ç”¨Advantageå‡½æ•°æ¥æ›´æ–°ç­–ç•¥ã€‚è¯¥ç®—æ³•å®ç°ç®€å•ï¼Œå¯ä»¥å¤„ç†ç¦»æ•£å’Œè¿ç»­çš„åŠ¨ä½œç©ºé—´ã€‚</span></p><section><pre><code><span>import</span>Â numpyÂ <span>as</span>Â np<br><span>from</span>Â keras.modelsÂ <span>import</span>Â Model,Â Sequential<br><span>from</span>Â keras.layersÂ <span>import</span>Â Dense,Â Input<br><span>from</span>Â keras.optimizersÂ <span>import</span>Â Adam<br><span>from</span>Â keras.utilsÂ <span>import</span>Â to_categorical<br><br><span>#Â DefineÂ theÂ actorÂ andÂ criticÂ models</span><br>state_inputÂ =Â Input(shape=(state_space_size,))<br>actorÂ =Â Dense(<span>32</span>,Â activation=<span>'relu'</span>)(state_input)<br>actorÂ =Â Dense(<span>32</span>,Â activation=<span>'relu'</span>)(actor)<br>actorÂ =Â Dense(action_space_size,Â activation=<span>'softmax'</span>)(actor)<br>actor_modelÂ =Â Model(inputs=state_input,Â outputs=actor)<br>actor_model.compile(loss=<span>'categorical_crossentropy'</span>,Â optimizer=Adam(lr=<span>0.001</span>))<br><br>state_inputÂ =Â Input(shape=(state_space_size,))<br>criticÂ =Â Dense(<span>32</span>,Â activation=<span>'relu'</span>)(state_input)<br>criticÂ =Â Dense(<span>32</span>,Â activation=<span>'relu'</span>)(critic)<br>criticÂ =Â Dense(<span>1</span>,Â activation=<span>'linear'</span>)(critic)<br>critic_modelÂ =Â Model(inputs=state_input,Â outputs=critic)<br>critic_model.compile(loss=<span>'mse'</span>,Â optimizer=Adam(lr=<span>0.001</span>))<br><br><span>for</span>Â episodeÂ <span>in</span>Â range(num_episodes):<br>Â Â Â Â current_stateÂ =Â initial_state<br>Â Â Â Â doneÂ =Â <span>False</span><br>Â Â Â Â <span>while</span>Â <span>not</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â SelectÂ anÂ actionÂ usingÂ theÂ actorÂ modelÂ andÂ addÂ explorationÂ noise</span><br>Â Â Â Â Â Â Â Â action_probsÂ =Â actor_model.predict(np.array([current_state]))[<span>0</span>]<br>Â Â Â Â Â Â Â Â actionÂ =Â np.random.choice(range(action_space_size),Â p=action_probs)<br><br>Â Â Â Â Â Â Â Â <span>#Â TakeÂ theÂ actionÂ andÂ observeÂ theÂ nextÂ stateÂ andÂ reward</span><br>Â Â Â Â Â Â Â Â next_state,Â reward,Â doneÂ =Â take_action(current_state,Â action)<br><br>Â Â Â Â Â Â Â Â <span>#Â CalculateÂ theÂ advantage</span><br>Â Â Â Â Â Â Â Â target_valueÂ =Â critic_model.predict(np.array([next_state]))[<span>0</span>][<span>0</span>]<br>Â Â Â Â Â Â Â Â advantageÂ =Â rewardÂ +Â gammaÂ *Â target_valueÂ -Â critic_model.predict(np.array([current_state]))[<span>0</span>][<span>0</span>]<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ actorÂ model</span><br>Â Â Â Â Â Â Â Â action_one_hotÂ =Â to_categorical(action,Â action_space_size)<br>Â Â Â Â Â Â Â Â actor_model.train_on_batch(np.array([current_state]),Â advantageÂ *Â action_one_hot)<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ criticÂ model</span><br>Â Â Â Â Â Â Â Â critic_model.train_on_batch(np.array([current_state]),Â rewardÂ +Â gammaÂ *Â target_value)<br><br>Â Â Â Â Â Â Â Â current_stateÂ =Â next_state<br></code></pre></section><p cid="n31" mdtype="paragraph"><span>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œactoræ¨¡å‹æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒæœ‰2ä¸ªéšè—å±‚ï¼Œæ¯ä¸ªéšè—å±‚æœ‰32ä¸ªç¥ç»å…ƒï¼Œå…·æœ‰reluæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºå±‚å…·æœ‰softmaxæ¿€æ´»å‡½æ•°ã€‚</span><span>criticæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒæœ‰2ä¸ªéšå«å±‚ï¼Œæ¯å±‚32ä¸ªç¥ç»å…ƒï¼Œå…·æœ‰reluæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºå±‚å…·æœ‰çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚</span></p><p cid="n84" mdtype="paragraph"><span md-inline="plain">ä½¿ç”¨åˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°è®­ç»ƒactoræ¨¡å‹ï¼Œä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°è®­ç»ƒcriticæ¨¡å‹ã€‚åŠ¨ä½œæ˜¯æ ¹æ®actoræ¨¡å‹é¢„æµ‹é€‰æ‹©çš„ï¼Œå¹¶æ·»åŠ äº†ç”¨äºæ¢ç´¢çš„å™ªå£°ã€‚</span></p><h2 cid="n72" mdtype="heading"><span md-inline="plain">5ã€PPO</span></h2><p cid="n74" mdtype="paragraph"><span md-inline="plain">PPOï¼ˆProximal Policy Optimizationï¼‰æ˜¯ä¸€ç§ç­–ç•¥ç®—æ³•ï¼Œå®ƒä½¿ç”¨ä¿¡ä»»åŸŸä¼˜åŒ–çš„æ–¹æ³•æ¥æ›´æ–°ç­–ç•¥ã€‚å®ƒåœ¨å…·æœ‰é«˜ç»´è§‚å¯Ÿå’Œè¿ç»­åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚PPO ä»¥å…¶ç¨³å®šæ€§å’Œé«˜æ ·å“æ•ˆç‡è€Œè‘—ç§°ã€‚</span></p><section><pre><code><span>import</span>Â numpyÂ <span>as</span>Â np<br><span>from</span>Â keras.modelsÂ <span>import</span>Â Model,Â Sequential<br><span>from</span>Â keras.layersÂ <span>import</span>Â Dense,Â Input<br><span>from</span>Â keras.optimizersÂ <span>import</span>Â Adam<br><br><span>#Â DefineÂ theÂ policyÂ model</span><br>state_inputÂ =Â Input(shape=(state_space_size,))<br>policyÂ =Â Dense(<span>32</span>,Â activation=<span>'relu'</span>)(state_input)<br>policyÂ =Â Dense(<span>32</span>,Â activation=<span>'relu'</span>)(policy)<br>policyÂ =Â Dense(action_space_size,Â activation=<span>'softmax'</span>)(policy)<br>policy_modelÂ =Â Model(inputs=state_input,Â outputs=policy)<br><br><span>#Â DefineÂ theÂ valueÂ model</span><br>value_modelÂ =Â Model(inputs=state_input,Â outputs=Dense(<span>1</span>,Â activation=<span>'linear'</span>)(policy))<br><br><span>#Â DefineÂ theÂ optimizer</span><br>optimizerÂ =Â Adam(lr=<span>0.001</span>)<br><br><span>for</span>Â episodeÂ <span>in</span>Â range(num_episodes):<br>Â Â Â Â current_stateÂ =Â initial_state<br>Â Â Â Â <span>while</span>Â <span>not</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â SelectÂ anÂ actionÂ usingÂ theÂ policyÂ model</span><br>Â Â Â Â Â Â Â Â action_probsÂ =Â policy_model.predict(np.array([current_state]))[<span>0</span>]<br>Â Â Â Â Â Â Â Â actionÂ =Â np.random.choice(range(action_space_size),Â p=action_probs)<br><br>Â Â Â Â Â Â Â Â <span>#Â TakeÂ theÂ actionÂ andÂ observeÂ theÂ nextÂ stateÂ andÂ reward</span><br>Â Â Â Â Â Â Â Â next_state,Â reward,Â doneÂ =Â take_action(current_state,Â action)<br><br>Â Â Â Â Â Â Â Â <span>#Â CalculateÂ theÂ advantage</span><br>Â Â Â Â Â Â Â Â target_valueÂ =Â value_model.predict(np.array([next_state]))[<span>0</span>][<span>0</span>]<br>Â Â Â Â Â Â Â Â advantageÂ =Â rewardÂ +Â gammaÂ *Â target_valueÂ -Â value_model.predict(np.array([current_state]))[<span>0</span>][<span>0</span>]<br><br>Â Â Â Â Â Â Â Â <span>#Â CalculateÂ theÂ oldÂ andÂ newÂ policyÂ probabilities</span><br>Â Â Â Â Â Â Â Â old_policy_probÂ =Â action_probs[action]<br>Â Â Â Â Â Â Â Â new_policy_probÂ =Â policy_model.predict(np.array([next_state]))[<span>0</span>][action]<br><br>Â Â Â Â Â Â Â Â <span>#Â CalculateÂ theÂ ratioÂ andÂ theÂ surrogateÂ loss</span><br>Â Â Â Â Â Â Â Â ratioÂ =Â new_policy_probÂ /Â old_policy_prob<br>Â Â Â Â Â Â Â Â surrogate_lossÂ =Â np.minimum(ratioÂ *Â advantage,Â np.clip(ratio,Â <span>1</span>Â -Â epsilon,Â <span>1</span>Â +Â epsilon)Â *Â advantage)<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ policyÂ andÂ valueÂ models</span><br>Â Â Â Â Â Â Â Â policy_model.trainable_weightsÂ =Â value_model.trainable_weights<br>Â Â Â Â Â Â Â Â policy_model.compile(optimizer=optimizer,Â loss=-surrogate_loss)<br>Â Â Â Â Â Â Â Â policy_model.train_on_batch(np.array([current_state]),Â np.array([action_one_hot]))<br>Â Â Â Â Â Â Â Â value_model.train_on_batch(np.array([current_state]),Â rewardÂ +Â gammaÂ *Â target_value)<br><br>Â Â Â Â Â Â Â Â current_stateÂ =Â next_state</code></pre></section><h2 cid="n80" mdtype="heading"><span md-inline="plain">6ã€DQN</span></h2><p cid="n92" mdtype="paragraph"><span md-inline="plain">DQNï¼ˆæ·±åº¦ Q ç½‘ç»œï¼‰æ˜¯ä¸€ç§æ— æ¨¡å‹ã€éç­–ç•¥ç®—æ³•ï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥é€¼è¿‘ Q å‡½æ•°ã€‚DQN ç‰¹åˆ«é€‚ç”¨äº Atari æ¸¸æˆå’Œå…¶ä»–ç±»ä¼¼é—®é¢˜ï¼Œå…¶ä¸­çŠ¶æ€ç©ºé—´æ˜¯é«˜ç»´çš„ï¼Œå¹¶ä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼ Q å‡½æ•°ã€‚</span></p><section><pre><code><span>import</span>Â numpyÂ <span>as</span>Â np<br><span>from</span>Â keras.modelsÂ <span>import</span>Â Sequential<br><span>from</span>Â keras.layersÂ <span>import</span>Â Dense,Â Input<br><span>from</span>Â keras.optimizersÂ <span>import</span>Â Adam<br><span>from</span>Â collectionsÂ <span>import</span>Â deque<br><br><span>#Â DefineÂ theÂ Q-networkÂ model</span><br>modelÂ =Â Sequential()<br>model.add(Dense(<span>32</span>,Â input_dim=state_space_size,Â activation=<span>'relu'</span>))<br>model.add(Dense(<span>32</span>,Â activation=<span>'relu'</span>))<br>model.add(Dense(action_space_size,Â activation=<span>'linear'</span>))<br>model.compile(loss=<span>'mse'</span>,Â optimizer=Adam(lr=<span>0.001</span>))<br><br><span>#Â DefineÂ theÂ replayÂ buffer</span><br>replay_bufferÂ =Â deque(maxlen=replay_buffer_size)<br><br><span>for</span>Â episodeÂ <span>in</span>Â range(num_episodes):<br>Â Â Â Â current_stateÂ =Â initial_state<br>Â Â Â Â <span>while</span>Â <span>not</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â SelectÂ anÂ actionÂ usingÂ anÂ epsilon-greedyÂ policy</span><br>Â Â Â Â Â Â Â Â <span>if</span>Â np.random.rand()Â &lt;Â epsilon:<br>Â Â Â Â Â Â Â Â Â Â Â Â actionÂ =Â np.random.randint(<span>0</span>,Â action_space_size)<br>Â Â Â Â Â Â Â Â <span>else</span>:<br>Â Â Â Â Â Â Â Â Â Â Â Â actionÂ =Â np.argmax(model.predict(np.array([current_state]))[<span>0</span>])<br><br>Â Â Â Â Â Â Â Â <span>#Â TakeÂ theÂ actionÂ andÂ observeÂ theÂ nextÂ stateÂ andÂ reward</span><br>Â Â Â Â Â Â Â Â next_state,Â reward,Â doneÂ =Â take_action(current_state,Â action)<br><br>Â Â Â Â Â Â Â Â <span>#Â AddÂ theÂ experienceÂ toÂ theÂ replayÂ buffer</span><br>Â Â Â Â Â Â Â Â replay_buffer.append((current_state,Â action,Â reward,Â next_state,Â done))<br><br>Â Â Â Â Â Â Â Â <span>#Â SampleÂ aÂ batchÂ ofÂ experiencesÂ fromÂ theÂ replayÂ buffer</span><br>Â Â Â Â Â Â Â Â batchÂ =Â random.sample(replay_buffer,Â batch_size)<br><br>Â Â Â Â Â Â Â Â <span>#Â PrepareÂ theÂ inputsÂ andÂ targetsÂ forÂ theÂ Q-network</span><br>Â Â Â Â Â Â Â Â inputsÂ =Â np.array([x[<span>0</span>]Â <span>for</span>Â xÂ <span>in</span>Â batch])<br>Â Â Â Â Â Â Â Â targetsÂ =Â model.predict(inputs)<br>Â Â Â Â Â Â Â Â <span>for</span>Â i,Â (state,Â action,Â reward,Â next_state,Â done)Â <span>in</span>Â enumerate(batch):<br>Â Â Â Â Â Â Â Â Â Â Â Â <span>if</span>Â done:<br>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â targets[i,Â action]Â =Â reward<br>Â Â Â Â Â Â Â Â Â Â Â Â <span>else</span>:<br>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â targets[i,Â action]Â =Â rewardÂ +Â gammaÂ *Â np.max(model.predict(np.array([next_state]))[<span>0</span>])<br><br>Â Â Â Â Â Â Â Â <span>#Â UpdateÂ theÂ Q-network</span><br>Â Â Â Â Â Â Â Â model.train_on_batch(inputs,Â targets)<br><br>Â Â Â Â Â Â Â Â current_stateÂ =Â next_state<br></code></pre></section><p cid="n92" mdtype="paragraph"><span>ä¸Šé¢çš„ä»£ç ï¼ŒQ-networkæœ‰2ä¸ªéšè—å±‚ï¼Œæ¯ä¸ªéšè—å±‚æœ‰32ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨reluæ¿€æ´»å‡½æ•°ã€‚</span><span>è¯¥ç½‘ç»œä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°å’ŒAdamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒã€‚</span></p><h2 cid="n96" mdtype="heading"><span md-inline="plain">7ã€TRPO</span></h2><p cid="n100" mdtype="paragraph"><span md-inline="plain">TRPO ï¼ˆTrust Region Policy Optimizationï¼‰æ˜¯ä¸€ç§æ— æ¨¡å‹çš„ç­–ç•¥ç®—æ³•ï¼Œå®ƒä½¿ç”¨ä¿¡ä»»åŸŸä¼˜åŒ–æ–¹æ³•æ¥æ›´æ–°ç­–ç•¥ã€‚å®ƒåœ¨å…·æœ‰é«˜ç»´è§‚å¯Ÿå’Œè¿ç»­åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚</span></p><p cid="n103" mdtype="paragraph"><span md-inline="plain">TRPO æ˜¯ä¸€ä¸ªå¤æ‚çš„ç®—æ³•ï¼Œéœ€è¦å¤šä¸ªæ­¥éª¤å’Œç»„ä»¶æ¥å®ç°ã€‚TRPOä¸æ˜¯ç”¨å‡ è¡Œä»£ç å°±èƒ½å®ç°çš„ç®€å•ç®—æ³•ã€‚</span></p><p cid="n105" mdtype="paragraph"><span md-inline="plain">æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œä½¿ç”¨å®ç°äº†TRPOçš„ç°æœ‰åº“ï¼Œä¾‹å¦‚OpenAI Baselinesï¼Œå®ƒæä¾›äº†åŒ…æ‹¬TRPOåœ¨å†…çš„å„ç§é¢„å…ˆå®ç°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œã€‚</span></p><p cid="n106" mdtype="paragraph"><span md-inline="plain">è¦åœ¨OpenAI Baselinesä¸­ä½¿ç”¨TRPOï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…:</span></p><section><pre><code>pipÂ installÂ baselines<br></code></pre></section><p cid="n106" mdtype="paragraph"><span>ç„¶å</span><span>å¯ä»¥ä½¿ç”¨baselinesåº“ä¸­çš„trpo_mpiæ¨¡å—åœ¨ä½ çš„ç¯å¢ƒä¸­è®­ç»ƒTRPOä»£ç†ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„ä¾‹å­:</span></p><section><pre><code><span>import</span>Â gym<br><span>from</span>Â baselines.common.vec_env.dummy_vec_envÂ <span>import</span>Â DummyVecEnv<br><span>from</span>Â baselines.trpo_mpiÂ <span>import</span>Â trpo_mpi<br><br><span>#Â InitializeÂ theÂ environment</span><br>envÂ =Â gym.make(<span>"CartPole-v1"</span>)<br>envÂ =Â DummyVecEnv([<span>lambda</span>:Â env])<br><br><span>#Â DefineÂ theÂ policyÂ network</span><br>policy_fnÂ =Â mlp_policy<br><br><span>#Â TrainÂ theÂ TRPOÂ model</span><br>modelÂ =Â trpo_mpi.learn(env,Â policy_fn,Â max_iters=<span>1000</span>)<span></span></code></pre></section><p cid="n106" mdtype="paragraph"><span>æˆ‘ä»¬ä½¿ç”¨Gymåº“åˆå§‹åŒ–ç¯å¢ƒã€‚</span><span>ç„¶åå®šä¹‰ç­–ç•¥ç½‘ç»œï¼Œå¹¶è°ƒç”¨TRPOæ¨¡å—ä¸­çš„learn()å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ã€‚</span></p><p cid="n116" mdtype="paragraph"><span md-inline="plain">è¿˜æœ‰è®¸å¤šå…¶ä»–åº“ä¹Ÿæä¾›äº†TRPOçš„å®ç°ï¼Œä¾‹å¦‚TensorFlowã€PyTorchå’ŒRLLibã€‚ä¸‹é¢æ—¶ä¸€ä¸ªä½¿ç”¨TF 2.0å®ç°çš„æ ·ä¾‹</span></p><section><pre><code><span>import</span>Â tensorflowÂ <span>as</span>Â tf<br><span>import</span>Â gym<br><br><br><span>#Â DefineÂ theÂ policyÂ network</span><br><span><span>class</span>Â <span>PolicyNetwork</span><span>(tf.keras.Model)</span>:</span><br>Â Â Â Â <span><span>def</span>Â <span>__init__</span><span>(self)</span>:</span><br>Â Â Â Â Â Â Â Â super(PolicyNetwork,Â self).__init__()<br>Â Â Â Â Â Â Â Â self.dense1Â =Â tf.keras.layers.Dense(<span>16</span>,Â activation=<span>'relu'</span>)<br>Â Â Â Â Â Â Â Â self.dense2Â =Â tf.keras.layers.Dense(<span>16</span>,Â activation=<span>'relu'</span>)<br>Â Â Â Â Â Â Â Â self.dense3Â =Â tf.keras.layers.Dense(<span>1</span>,Â activation=<span>'sigmoid'</span>)<br><br>Â Â Â Â <span><span>def</span>Â <span>call</span><span>(self,Â inputs)</span>:</span><br>Â Â Â Â Â Â Â Â xÂ =Â self.dense1(inputs)<br>Â Â Â Â Â Â Â Â xÂ =Â self.dense2(x)<br>Â Â Â Â Â Â Â Â xÂ =Â self.dense3(x)<br>Â Â Â Â Â Â Â Â <span>return</span>Â x<br><br><br><span>#Â InitializeÂ theÂ environment</span><br>envÂ =Â gym.make(<span>"CartPole-v1"</span>)<br><br><span>#Â InitializeÂ theÂ policyÂ network</span><br>policy_networkÂ =Â PolicyNetwork()<br><br><span>#Â DefineÂ theÂ optimizer</span><br>optimizerÂ =Â tf.optimizers.Adam()<br><br><span>#Â DefineÂ theÂ lossÂ function</span><br>loss_fnÂ =Â tf.losses.BinaryCrossentropy()<br><br><span>#Â SetÂ theÂ maximumÂ numberÂ ofÂ iterations</span><br>max_itersÂ =Â <span>1000</span><br><br><span>#Â StartÂ theÂ trainingÂ loop</span><br><span>for</span>Â iÂ <span>in</span>Â range(max_iters):<br>Â Â Â Â <span>#Â SampleÂ anÂ actionÂ fromÂ theÂ policyÂ network</span><br>Â Â Â Â actionÂ =Â tf.squeeze(tf.random.categorical(policy_network(observation),Â <span>1</span>))<br><br>Â Â Â Â <span>#Â TakeÂ aÂ stepÂ inÂ theÂ environment</span><br>Â Â Â Â observation,Â reward,Â done,Â _Â =Â env.step(action)<br><br>Â Â Â Â <span>with</span>Â tf.GradientTape()Â <span>as</span>Â tape:<br>Â Â Â Â Â Â Â Â <span>#Â ComputeÂ theÂ loss</span><br>Â Â Â Â Â Â Â Â lossÂ =Â loss_fn(reward,Â policy_network(observation))<br><br>Â Â Â Â <span>#Â ComputeÂ theÂ gradients</span><br>Â Â Â Â gradsÂ =Â tape.gradient(loss,Â policy_network.trainable_variables)<br><br>Â Â Â Â <span>#Â PerformÂ theÂ updateÂ step</span><br>Â Â Â Â optimizer.apply_gradients(zip(grads,Â policy_network.trainable_variables))<br><br>Â Â Â Â <span>if</span>Â done:<br>Â Â Â Â Â Â Â Â <span>#Â ResetÂ theÂ environment</span><br>Â Â Â Â Â Â Â Â observationÂ =Â env.reset()<br></code></pre></section><p cid="n116" mdtype="paragraph"><span>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨TensorFlowçš„Keras APIå®šä¹‰ä¸€ä¸ªç­–ç•¥ç½‘ç»œã€‚</span><span>ç„¶åä½¿ç”¨Gymåº“å’Œç­–ç•¥ç½‘ç»œåˆå§‹åŒ–ç¯å¢ƒã€‚</span><span>ç„¶åå®šä¹‰ç”¨äºè®­ç»ƒç­–ç•¥ç½‘ç»œçš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ã€‚</span></p><p cid="n131" mdtype="paragraph"><span md-inline="plain">åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œä»ç­–ç•¥ç½‘ç»œä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œåœ¨ç¯å¢ƒä¸­å‰è¿›ä¸€æ­¥ï¼Œç„¶åä½¿ç”¨TensorFlowçš„GradientTapeè®¡ç®—æŸå¤±å’Œæ¢¯åº¦ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ä¼˜åŒ–å™¨æ‰§è¡Œæ›´æ–°æ­¥éª¤ã€‚</span></p><p cid="n132" mdtype="paragraph"><span md-inline="plain">è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œåªå±•ç¤ºäº†å¦‚ä½•åœ¨TensorFlow 2.0ä¸­å®ç°TRPOã€‚TRPOæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„ç®—æ³•ï¼Œè¿™ä¸ªä¾‹å­æ²¡æœ‰æ¶µç›–æ‰€æœ‰çš„ç»†èŠ‚ï¼Œä½†å®ƒæ˜¯è¯•éªŒTRPOçš„ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚</span></p><h2 cid="n117" mdtype="heading"><span md-inline="plain">æ€»ç»“</span></h2><p cid="n133" mdtype="paragraph"><span md-inline="plain">ä»¥ä¸Šå°±æ˜¯æˆ‘ä»¬æ€»ç»“çš„7ä¸ªå¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¿™äº›ç®—æ³•å¹¶ä¸ç›¸äº’æ’æ–¥ï¼Œé€šå¸¸ä¸å…¶ä»–æŠ€æœ¯(å¦‚å€¼å‡½æ•°é€¼è¿‘ã€åŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œé›†æˆæ–¹æ³•)ç»“åˆä½¿ç”¨ï¼Œå¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚</span><span></span></p><section><strong><span><strong><span>ä¸‡æ°´åƒå±±æ€»æ˜¯æƒ…ï¼Œç‚¹ä¸ªÂ ğŸ‘Â è¡Œä¸è¡Œ</span></strong>ã€‚</span></strong></section><section><br></section><section><br></section><section><span><strong>æ¨èé˜…è¯»</strong></span></section><section><br></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU4OTYzNjE2OQ==&amp;mid=2247484115&amp;idx=1&amp;sn=447d90d9e9d17b963c6dc76f7d691735&amp;chksm=fdcb35f5cabcbce3bb4318fa7271a367a2beca95e6aaa02670a61844c6497b6aa2a258588b90&amp;scene=21#wechat_redirect" target="_blank" data-linktype="1"><span data-positionback="static"><img data-copyright="0" data-ratio="0.36171875" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY28yiangUibwP8pQia0dVVRnyBEcB8NEeCgbtzSbGiaIhwlmSWVNAbQNkPPw/640?wx_fmt=jpeg" data-type="jpeg" data-w="1280" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY28yiangUibwP8pQia0dVVRnyBEcB8NEeCgbtzSbGiaIhwlmSWVNAbQNkPPw/640?wx_fmt=jpeg"></span></a></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU4OTYzNjE2OQ==&amp;mid=2247484095&amp;idx=1&amp;sn=a284ba324550829c969fa2cda0da723c&amp;chksm=fdcb3599cabcbc8f96df521eb5ce50910da152de627e47a69b26e9f2da5920dc6afe4ad2642f&amp;scene=21#wechat_redirect" target="_blank" data-linktype="1"><span data-positionback="static"><img data-copyright="0" data-ratio="0.3613963039014374" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY2NGWbAsoex55wOX7AaeSCTpaHYibpcv0btGvriaGxNlbHTsIAtVAiceg2w/640?wx_fmt=jpeg" data-type="jpeg" data-w="1948" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY2NGWbAsoex55wOX7AaeSCTpaHYibpcv0btGvriaGxNlbHTsIAtVAiceg2w/640?wx_fmt=jpeg"></span></a></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU4OTYzNjE2OQ==&amp;mid=2247484076&amp;idx=1&amp;sn=fac8b75e2fbdb8eecff16b1faa4dc7f2&amp;chksm=fdcb358acabcbc9c20769d3800f77f7cafb16844486e2c3fd35b0d567923eeac352189cc6285&amp;scene=21#wechat_redirect" target="_blank" data-linktype="1"><span data-positionback="static"><img data-copyright="0" data-ratio="0.3613963039014374" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakn3PDXd3AZqsndDkz0N8f1aOSbmwb81qkW1ZfyibRVP300ESWyibhjIicWuw5gV7gw3tSz2omeia6Ftw/640?wx_fmt=jpeg" data-type="jpeg" data-w="1948" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakn3PDXd3AZqsndDkz0N8f1aOSbmwb81qkW1ZfyibRVP300ESWyibhjIicWuw5gV7gw3tSz2omeia6Ftw/640?wx_fmt=jpeg"></span></a></section><section><br></section><section><strong><span>Â·Â·Â·Â  ENDÂ  Â·Â·Â·</span></strong></section><section><br></section><section><img data-copyright="0" data-ratio="0.4618249534450652" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakhhYibk7icTEGoX7VZ2R8g9CwnITeJvFL2R3IeQIenSuFibD9ibNO6AWXqMpgOWbS05P1vic3tafbP0qQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="1074" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakhhYibk7icTEGoX7VZ2R8g9CwnITeJvFL2R3IeQIenSuFibD9ibNO6AWXqMpgOWbS05P1vic3tafbP0qQ/640?wx_fmt=jpeg"></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/DAPirChUTKZ9yLExJw86Tg",target="_blank" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a>
