---
title: "知识扩展---图神经网络GraphSAGE用于空间转录组的分子聚类"
date: 2025-02-05T08:07:29Z
draft: ["false"]
tags: [
  "fetched",
  "单细胞空间交响乐"
]
categories: ["Acdemic"]
---
知识扩展---图神经网络GraphSAGE用于空间转录组的分子聚类 by 单细胞空间交响乐
------
<div><h4 data-line="0"><span leaf="">作者，Evil Genius</span></h4><h4 data-line="2"><span leaf="">开工第一天，岁数又加一。</span></h4><h4 data-line="4"><span leaf="">今天我们分享的内容是，GraphSAGE。</span></h4><h4 data-line="6"><span leaf="">在昨天的文章</span><span leaf=""></span><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MDY1NTYyOQ==&amp;mid=2247494551&amp;idx=1&amp;sn=7c2c43bcb96ef0f25a638f3c8f823386&amp;scene=21#wechat_redirect" textvalue="文献分享---空间转录组学鉴定与肺纤维化远端肺重构相关的分子生态位失调（Xenium + HD）" data-itemshowtype="0" target="_blank" linktype="text" data-linktype="2"><span textstyle="">文献分享---空间转录组学鉴定与肺纤维化远端肺重构相关的分子生态位失调（Xenium + HD）</span></a><span leaf="">中，我们可以看到文中对空间转录组的分子聚类采用的是</span></h4><section nodeleaf=""><img data-imgfileid="100010906" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OjlLYCK6Yhzzww4JXibGfBDGN8whHNtqRVyibx6Q37pA486icCqTj914Ag/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OjlLYCK6Yhzzww4JXibGfBDGN8whHNtqRVyibx6Q37pA486icCqTj914Ag/640?wx_fmt=other&amp;from=appmsg"></section><section><br></section><section nodeleaf=""><img data-imgfileid="100010914" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OXCS6n9ykiagjW1LyOBtauMZJMP9aTCqKcdbqxbibSX425H3WibiadvefSg/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OXCS6n9ykiagjW1LyOBtauMZJMP9aTCqKcdbqxbibSX425H3WibiadvefSg/640?wx_fmt=other&amp;from=appmsg"></section><section><br></section><section nodeleaf=""><img data-imgfileid="100010908" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OxhFtibiadwd0Z3ju0WVBjbXJZ8srOkzt3HQ470tO50XlSXDMPAxQCia3Q/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OxhFtibiadwd0Z3ju0WVBjbXJZ8srOkzt3HQ470tO50XlSXDMPAxQCia3Q/640?wx_fmt=other&amp;from=appmsg"></section><h4 data-line="11"><span leaf="">从文章的信息可以获知，1、GraphSAGE是一种图神经网络；2、GraphSAGE，<span textstyle="">基于转录组数据的空间位置训练了一个图神经网络模型以聚合局部邻域信息并定义一个嵌入空间，该空间为数据集中的所有单个转录组提供了新的表示。然后，应用高斯混合模型在嵌入空间中聚类转录组，并识别生态位，使用共识方法将细胞分配到这些生态位中。</span></span></h4><h4 data-line="13"><span leaf="">那么我们需要首先知道什么是图神经网络GraphSAGE。</span></h4><h4 data-line="15"><span leaf="">GraphSAGE（Graph Sample and Aggregated）是一种用于图节点嵌入学习的图神经网络模型。它通过<span textstyle="">采样和聚合</span>的方式，将邻居节点的信息聚合到目标节点上，从而学习节点的表示向量。</span></h4><h5 data-line="17"><span leaf="">GraphSAGE的核心思想是从目标节点的邻居节点中采样一部分节点，然后通过聚合操作将邻居节点的特征信息整合到目标节点上。这样一方面减少了计算复杂度，另一方面也保留了图结构中的信息。GraphSAGE模型包含以下几个重要的步骤：</span></h5><blockquote><p><strong><span leaf=""><span textstyle="">采样</span></span></strong><span leaf=""><span textstyle="">：</span>针对每个节点，从其邻居节点中随机采样一定数量的节点作为采样节点集合。采样的目的是探索节点的局部结构，以便更好地捕捉节点的特征。</span><br><strong><span leaf=""><span textstyle="">编码器</span></span></strong><span leaf=""><span textstyle="">：</span>对于每个节点，通过一个编码器将其自身的特征向量转换为一个低维的表示向量。编码器可以是一个全连接层、一个卷积神经网络等。</span><br><strong><span leaf=""><span textstyle="">聚合器</span></span></strong><span leaf=""><span textstyle="">：</span>通过聚合操作将采样节点集合中的特征向量聚合到目标节点上。常用的聚合方法有均值聚合、最大池化等。聚合的过程可以通过多层的聚合器进行迭代。</span></p></blockquote><h5 data-line="22"><span leaf="">通过多层的编码器和聚合器，GraphSAGE能够逐渐聚合更多层次的邻居节点信息，并且逐渐扩大目标节点对邻居节点的感知范围。最终，每个节点都能够获得一个表示其自身和周围结构的嵌入向量，该向量可以用于下游的节点分类、链接预测等任务。</span></h5><h5 data-line="24"><span leaf="">GraphSAGE在图节点嵌入学习任务中具有较好的性能，能够有效地学习图结构中的节点特征。它可以用于社交网络分析、推荐系统、图像分析等领域，对于挖掘和分析图结构数据具有重要的应用价值。</span></h5><h4 data-line="26"><span leaf="">来到这个地方，我们再扩展一下</span></h4><h5 data-line="27"><span leaf="">在大型图中，节点的低维向量embedding被证明了作为各种各样的预测和图分析任务的特征输入是非常有用的。顶点embedding最基本的基本思想是使用降维技术从高维信息中提炼一个顶点的邻居信息，存到低维向量中。这些顶点嵌入之后会作为后续的机器学习系统的输入，解决像顶点分类、聚类、链接预测这样的问题。</span></h5><h5 data-line="28"><span leaf="">GCN虽然能提取图中顶点的embedding，但是存在一些问题：</span></h5><h6 data-line="29"><span leaf="">GCN的基本思想： 把一个节点在图中的高纬度邻接信息降维到一个低维的向量表示。</span></h6><h6 data-line="30"><span leaf="">GCN的优点： 可以捕捉graph的全局信息，从而很好地表示node的特征。</span></h6><h6 data-line="31"><span leaf="">GCN的缺点： 第一，GCN需要将整个图放到内存和显存，这将非常耗内存和显存，处理不了大图；第二，GCN在训练时需要知道整个图的结构信息(包括待预测的节点), 这在现实某些任务中也不能实现(比如用今天训练的图模型预测明天的数据，那么明天的节点是拿不到的)。</span></h6><h4 data-line="33"><span leaf="">为了解决GCN的两个缺点问题，GraphSAGE被提了出来。在介绍GraphSAGE之前，先介绍一下Inductive learning和Transductive learning。注意到图数据和其他类型数据的不同，图数据中的每一个节点可以通过边的关系利用其他节点的信息。这就导致一个问题，GCN输入了整个图，训练节点收集邻居节点信息的时候，用到了测试和验证集的样本，我们把这个称为Transductive learning。然而，我们所处理的大多数的机器学习问题都是Inductive learning，因为我们刻意的将样本集分为训练/验证/测试，并且训练的时候只用训练样本。这样对图来说有个好处，可以处理图中新来的节点，可以利用已知节点的信息为未知节点生成embedding，GraphSAGE就是这么干的。</span></h4><h4 data-line="35"><span leaf="">GraphSAGE是一个Inductive Learning框架，具体实现中，训练时它仅仅保留训练样本到训练样本的边，然后包含Sample和Aggregate两大步骤，Sample是指如何对邻居的个数进行采样，Aggregate是指拿到邻居节点的embedding之后如何汇聚这些embedding以更新自己的embedding信息。下图展示了GraphSAGE学习的一个过程：</span></h4><section nodeleaf=""><img data-imgfileid="100010904" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OuN8kiaZlqiaTM7ibQbbjLWLwo6DorHKAv01MQ8SZf4tU6KBickEu1mYaGg/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OuN8kiaZlqiaTM7ibQbbjLWLwo6DorHKAv01MQ8SZf4tU6KBickEu1mYaGg/640?wx_fmt=other&amp;from=appmsg"></section><p><span leaf=""><span textstyle="">图1</span></span></p><section><br></section><section nodeleaf=""><img data-imgfileid="100010905" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OQUjKvpVPBln3CwcKkZmrROt6O7P4MvMRxRtbrn4sdicphGm5rGmJLdw/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OQUjKvpVPBln3CwcKkZmrROt6O7P4MvMRxRtbrn4sdicphGm5rGmJLdw/640?wx_fmt=other&amp;from=appmsg"></section><h4 data-line="39"><span leaf="">简单的逻辑过程如下</span></h4><h5 data-line="40"><span leaf=""><span textstyle="">第一步，对邻居采样；</span></span></h5><h5 data-line="41"><span leaf=""><span textstyle="">第二步，采样后的邻居embedding传到节点上来，并使用一个聚合函数聚合这些邻居信息以更新节点的embedding；</span></span></h5><h5 data-line="42"><span leaf=""><span textstyle="">第三步，根据更新后的embedding预测节点的标签。</span></span></h5><h4 data-line="44"><span leaf="">算法假设模型已经过训练，参数是固定的。</span></h4><section nodeleaf=""><img data-imgfileid="100010910" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OjR2msmPNAsqU9em9Jx1Ej3z5wREHic0lS8s1SmuFcx0mNbb8DpzX8kg/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OjR2msmPNAsqU9em9Jx1Ej3z5wREHic0lS8s1SmuFcx0mNbb8DpzX8kg/640?wx_fmt=other&amp;from=appmsg"></section><h5 data-line="47"><span leaf="">算法是，在每次迭代(或搜索深度)，顶点从它们的局部邻居聚合信息，并且随着这个过程的迭代，顶点会从越来越远的地方获得信息。</span></h5><h5 data-line="49"><span leaf=""><span textstyle="">到这里还有多少没有阵亡的小伙伴？</span></span></h5><h4 data-line="51"><span leaf="">算法描述了在整个图上生成embedding的过程，其中</span></h4><section nodeleaf=""><img data-imgfileid="100010909" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OLzsQAbVGnFziaCgOmnwSQia5kq6tH3Xxw6MQuU8dKPgVVad3yanVibibiaA/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OLzsQAbVGnFziaCgOmnwSQia5kq6tH3Xxw6MQuU8dKPgVVad3yanVibibiaA/640?wx_fmt=other&amp;from=appmsg"></section><h5 data-line="54"><span leaf="">首先，(line1)算法首先初始化输入的图中所有节点的特征向量，(line3)对于每个节点 ，拿到它采样后的邻居节点 后，(line4)利用聚合函数聚合邻居节点的信息，(line5)并结合自身embedding通过一个非线性变换更新自身的embedding表示。</span></h5><h5 data-line="56"><span leaf="">注意到算法里面的 ，它是指聚合器的数量，也是指权重矩阵的数量，还是网络的层数，这是因为每一层网络中聚合器和权重矩阵是共享的。网络的层数可以理解为需要最大访问的邻居的跳数(hops)，比如在图1中，红色节点的更新拿到了它一、二跳邻居的信息，那么网络层数就是2。为了更新红色节点，首先在第一层(k=1)，会将蓝色节点的信息聚合到红色解节点上，将绿色节点的信息聚合到蓝色节点上。在第二层(k=2)红色节点的embedding被再次更新，不过这次用到的是更新后的蓝色节点embedding，这样就保证了红色节点更新后的embedding包括蓝色和绿色节点的信息，也就是两跳信息。</span></h5><section nodeleaf=""><img data-imgfileid="100010913" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5O7nTXUXIYmjREVByTlRxFh9G5Vetaia5TzzugZOkgQJGiapZbS3aicUXGA/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5O7nTXUXIYmjREVByTlRxFh9G5Vetaia5TzzugZOkgQJGiapZbS3aicUXGA/640?wx_fmt=other&amp;from=appmsg"></section><section><br></section><section nodeleaf=""><img data-imgfileid="100010912" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OFIeib6Uia4udxTXhoe3SJRmkNaMdHCxqYsVm1ruJicurPEk1hBcFd6JCg/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OFIeib6Uia4udxTXhoe3SJRmkNaMdHCxqYsVm1ruJicurPEk1hBcFd6JCg/640?wx_fmt=other&amp;from=appmsg"></section><h4 data-line="60"><span leaf="">最后来看看示例代码</span></h4><h5 data-line="61"><span leaf="">首先采样</span></h5><pre><code><span><span leaf="">import</span></span><span leaf=""> numpy </span><span><span leaf="">as</span></span><span leaf=""> np</span><br><br><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">sampling</span></span><span><span leaf="">(src_nodes, sample_num, neighbor_table)</span></span><span leaf="">:</span></span><br><span leaf="">    </span><span><span leaf="">"""根据源节点采样指定数量的邻居节点，注意使用的是有放回的采样；</span><br><span leaf="">    某个节点的邻居节点数量少于采样数量时，采样结果出现重复的节点</span><br><span leaf="">    </span><br><span leaf="">    Arguments:</span><br><span leaf="">        src_nodes {list, ndarray} -- 源节点列表</span><br><span leaf="">        sample_num {int} -- 需要采样的节点数</span><br><span leaf="">        neighbor_table {dict} -- 节点到其邻居节点的映射表</span><br><span leaf="">    </span><br><span leaf="">    Returns:</span><br><span leaf="">        np.ndarray -- 采样结果构成的列表</span><br><span leaf="">    """</span></span><br><span leaf="">    results = []</span><br><span leaf="">    </span><span><span leaf="">for</span></span><span leaf=""> sid </span><span><span leaf="">in</span></span><span leaf=""> src_nodes:</span><br><span leaf="">        </span><span><span leaf=""># 从节点的邻居中进行有放回地进行采样</span></span><br><span leaf="">        res = np.random.choice(neighbor_table[sid], size=(sample_num, ))</span><br><span leaf="">        results.append(res)</span><br><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> np.asarray(results).flatten()</span><br><br><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">multihop_sampling</span></span><span><span leaf="">(src_nodes, sample_nums, neighbor_table)</span></span><span leaf="">:</span></span><br><span leaf="">    </span><span><span leaf="">"""根据源节点进行多阶采样</span><br><span leaf="">    </span><br><span leaf="">    Arguments:</span><br><span leaf="">        src_nodes {list, np.ndarray} -- 源节点id</span><br><span leaf="">        sample_nums {list of int} -- 每一阶需要采样的个数</span><br><span leaf="">        neighbor_table {dict} -- 节点到其邻居节点的映射</span><br><span leaf="">    </span><br><span leaf="">    Returns:</span><br><span leaf="">        [list of ndarray] -- 每一阶采样的结果</span><br><span leaf="">    """</span></span><br><span leaf="">    sampling_result = [src_nodes]</span><br><span leaf="">    </span><span><span leaf="">for</span></span><span leaf=""> k, hopk_num </span><span><span leaf="">in</span></span><span leaf=""> enumerate(sample_nums):</span><br><span leaf="">        hopk_result = sampling(sampling_result[k], hopk_num, neighbor_table)</span><br><span leaf="">        sampling_result.append(hopk_result)</span><br><span leaf="">    </span><span><span leaf="">return</span></span><span leaf=""> sampling_result</span><br></code></pre><h5 data-line="101"><span leaf="">然后聚合</span></h5><p data-line="102"><span leaf="">聚合通过神经网络来实现。在定义的forward函数中，输入neighbor_feature表示需要聚合的邻居节点的特征，它的维度为 N</span><sub><span leaf="">src</span></sub><span leaf=""> × N</span><sub><span leaf="">neighbor</span></sub><span leaf=""> × D</span><sub><span leaf="">in</span></sub><span leaf="">，其中 N</span><sub><span leaf="">src</span></sub><span leaf="">表示源节点的数量， N</span><sub><span leaf="">neighbor</span></sub><span leaf="">表示邻居节点的数量， D</span><sub><span leaf="">in</span></sub><span leaf="">表示输入特征的维度。将这些邻居节点的特征通过一个线性变换得到隐藏层特征，进而进行求和、均值、最大值等聚合操作，得到N</span><sub><span leaf="">src</span></sub><span leaf=""> × N</span><sub><span leaf="">neighbor</span></sub><span leaf=""> × D</span><sub><span leaf="">in</span></sub><span leaf="">维的输出。</span></p><pre><code><span leaf="">import torch</span><br><span leaf="">import torch.nn as nn</span><br><span leaf="">import torch.nn.functional as F</span><br><span leaf="">import torch.nn.init as init</span><br><br><span><span><span leaf="">class</span></span><span leaf=""> </span><span><span leaf="">NeighborAggregator</span></span><span leaf="">(</span><span><span leaf="">nn</span></span><span leaf="">.</span><span><span leaf="">Module</span></span><span leaf="">):</span></span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">__init__</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">, input_dim, output_dim, </span><br><span leaf="">                 use_bias=False, aggr_method=</span><span><span leaf="">"mean"</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        </span><span><span leaf="">""</span></span><span><span leaf="">"聚合节点邻居</span><br><span leaf="">        Args:</span><br><span leaf="">            input_dim: 输入特征的维度</span><br><span leaf="">            output_dim: 输出特征的维度</span><br><span leaf="">            use_bias: 是否使用偏置 (default: {False})</span><br><span leaf="">            aggr_method: 邻居聚合方式 (default: {mean})</span><br><span leaf="">        "</span></span><span><span leaf="">""</span></span><br><span leaf="">        </span><span><span leaf="">super</span></span><span leaf="">(NeighborAggregator, </span><span><span leaf="">self</span></span><span leaf="">).__init_</span><span><span leaf="">_</span></span><span leaf="">()</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.input_dim = input_dim</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.output_dim = output_dim</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.use_bias = use_bias</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.aggr_method = aggr_method</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.</span><span><span leaf="">use_bias:</span></span><br><span leaf="">            </span><span><span leaf="">self</span></span><span leaf="">.bias = nn.Parameter(torch.Tensor(</span><span><span leaf="">self</span></span><span leaf="">.output_dim))</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.reset_parameters()</span><br><span leaf="">    </span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">reset_parameters</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        init.kaiming_uniform</span><span><span leaf="">_</span></span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">.weight)</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.</span><span><span leaf="">use_bias:</span></span><br><span leaf="">            init.zeros</span><span><span leaf="">_</span></span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">.bias)</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">forward</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">, neighbor_feature)</span></span></span><span leaf="">:</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.aggr_method == </span><span><span leaf="">"mean"</span></span><span leaf="">:</span><br><span leaf="">            aggr_neighbor = neighbor_feature.mean(dim=</span><span><span leaf="">1</span></span><span leaf="">)</span><br><span leaf="">        elif </span><span><span leaf="">self</span></span><span leaf="">.aggr_method == </span><span><span leaf="">"sum"</span></span><span leaf="">:</span><br><span leaf="">            aggr_neighbor = neighbor_feature.sum(dim=</span><span><span leaf="">1</span></span><span leaf="">)</span><br><span leaf="">        elif </span><span><span leaf="">self</span></span><span leaf="">.aggr_method == </span><span><span leaf="">"max"</span></span><span leaf="">:</span><br><span leaf="">            aggr_neighbor = neighbor_feature.max(dim=</span><span><span leaf="">1</span></span><span leaf="">)</span><br><span leaf="">        </span><span><span leaf="">else:</span></span><br><span leaf="">            raise ValueError(</span><span><span leaf="">"Unknown aggr type, expected sum, max, or mean, but got {}"</span></span><br><span leaf="">                             .format(</span><span><span leaf="">self</span></span><span leaf="">.aggr_method))</span><br><span leaf="">        </span><br><span leaf="">        neighbor_hidden = torch.matmul(aggr_neighbor, </span><span><span leaf="">self</span></span><span leaf="">.weight)</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.</span><span><span leaf="">use_bias:</span></span><br><span leaf="">            neighbor_hidden += </span><span><span leaf="">self</span></span><span leaf="">.bias</span><br><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> neighbor_hidden</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">extra_repr</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> </span><span><span leaf="">'in_features={}, out_features={}, aggr_method={}'</span></span><span leaf="">.format(</span><br><span leaf="">            </span><span><span leaf="">self</span></span><span leaf="">.input_dim, </span><span><span leaf="">self</span></span><span leaf="">.output_dim, </span><span><span leaf="">self</span></span><span leaf="">.aggr_method)</span><br></code></pre><h5 data-line="156"><span leaf="">GraphSAGE模型构建(net.py)</span></h5><section nodeleaf=""><img data-imgfileid="100010911" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OWy4181RPxFic7LvtEl6KRb6DPRa875GYxhC1HmPI94yKib18dYp5LnIg/640?wx_fmt=other&amp;from=appmsg" data-type="other" src="https://mmbiz.qpic.cn/mmbiz_jpg/srXAibe95Mmltc4Ws3GasbpP9ibZyJSH5OWy4181RPxFic7LvtEl6KRb6DPRa875GYxhC1HmPI94yKib18dYp5LnIg/640?wx_fmt=other&amp;from=appmsg"></section><pre><code><span><span><span leaf="">class</span></span><span leaf=""> </span><span><span leaf="">SageGCN</span></span><span leaf="">(</span><span><span leaf="">nn</span></span><span leaf="">.</span><span><span leaf="">Module</span></span><span leaf="">):</span></span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">__init__</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">, input_dim, hidden_dim,</span><br><span leaf="">                 activation=F.relu,</span><br><span leaf="">                 aggr_neighbor_method=</span><span><span leaf="">"mean"</span></span><span leaf="">,</span><br><span leaf="">                 aggr_hidden_method=</span><span><span leaf="">"sum"</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        </span><span><span leaf="">""</span></span><span><span leaf="">"SageGCN层定义</span><br><span leaf="">        Args:</span><br><span leaf="">            input_dim: 输入特征的维度</span><br><span leaf="">            hidden_dim: 隐层特征的维度，</span><br><span leaf="">                当aggr_hidden_method=sum, 输出维度为hidden_dim</span><br><span leaf="">                当aggr_hidden_method=concat, 输出维度为hidden_dim*2</span><br><span leaf="">            activation: 激活函数</span><br><span leaf="">            aggr_neighbor_method: 邻居特征聚合方法，["</span></span><span leaf="">mean</span><span><span leaf="">", "</span></span><span leaf="">sum</span><span><span leaf="">", "</span></span><span leaf="">max</span><span><span leaf="">"]</span><br><span leaf="">            aggr_hidden_method: 节点特征的更新方法，["</span></span><span leaf="">sum</span><span><span leaf="">", "</span></span><span leaf="">concat</span><span><span leaf="">"]</span><br><span leaf="">        "</span></span><span><span leaf="">""</span></span><br><span leaf="">        </span><span><span leaf="">super</span></span><span leaf="">(SageGCN, </span><span><span leaf="">self</span></span><span leaf="">).__init_</span><span><span leaf="">_</span></span><span leaf="">()</span><br><span leaf="">        assert aggr_neighbor_method </span><span><span leaf="">in</span></span><span leaf=""> [</span><span><span leaf="">"mean"</span></span><span leaf="">, </span><span><span leaf="">"sum"</span></span><span leaf="">, </span><span><span leaf="">"max"</span></span><span leaf="">]</span><br><span leaf="">        assert aggr_hidden_method </span><span><span leaf="">in</span></span><span leaf=""> [</span><span><span leaf="">"sum"</span></span><span leaf="">, </span><span><span leaf="">"concat"</span></span><span leaf="">]</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.input_dim = input_dim</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.hidden_dim = hidden_dim</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.aggr_neighbor_method = aggr_neighbor_method</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.aggr_hidden_method = aggr_hidden_method</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.activation = activation</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.aggregator = NeighborAggregator(input_dim, hidden_dim,</span><br><span leaf="">                                             aggr_method=aggr_neighbor_method)</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.b = nn.Parameter(torch.Tensor(input_dim, hidden_dim))</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.reset_parameters()</span><br><span leaf="">    </span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">reset_parameters</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        init.kaiming_uniform</span><span><span leaf="">_</span></span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">.b)</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">forward</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">, src_node_features, neighbor_node_features)</span></span></span><span leaf="">:</span><br><span leaf="">        neighbor_hidden = </span><span><span leaf="">self</span></span><span leaf="">.aggregator(neighbor_node_features)</span><br><span leaf="">        self_hidden = torch.matmul(src_node_features, </span><span><span leaf="">self</span></span><span leaf="">.b)</span><br><span leaf="">        </span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.aggr_hidden_method == </span><span><span leaf="">"sum"</span></span><span leaf="">:</span><br><span leaf="">            hidden = self_hidden + neighbor_hidden</span><br><span leaf="">        elif </span><span><span leaf="">self</span></span><span leaf="">.aggr_hidden_method == </span><span><span leaf="">"concat"</span></span><span leaf="">:</span><br><span leaf="">            hidden = torch.cat([self_hidden, neighbor_hidden], dim=</span><span><span leaf="">1</span></span><span leaf="">)</span><br><span leaf="">        </span><span><span leaf="">else:</span></span><br><span leaf="">            raise ValueError(</span><span><span leaf="">"Expected sum or concat, got {}"</span></span><br><span leaf="">                             .format(</span><span><span leaf="">self</span></span><span leaf="">.aggr_hidden))</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.</span><span><span leaf="">activation:</span></span><br><span leaf="">            </span><span><span leaf="">return</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.activation(hidden)</span><br><span leaf="">        </span><span><span leaf="">else:</span></span><br><span leaf="">            </span><span><span leaf="">return</span></span><span leaf=""> hidden</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">extra_repr</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        output_dim = </span><span><span leaf="">self</span></span><span leaf="">.hidden_dim </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.aggr_hidden_method == </span><span><span leaf="">"sum"</span></span><span leaf=""> </span><span><span leaf="">else</span></span><span leaf=""> </span><span><span leaf="">self</span></span><span leaf="">.hidden_dim * </span><span><span leaf="">2</span></span><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> </span><span><span leaf="">'in_features={}, out_features={}, aggr_hidden_method={}'</span></span><span leaf="">.format(</span><br><span leaf="">            </span><span><span leaf="">self</span></span><span leaf="">.input_dim, output_dim, </span><span><span leaf="">self</span></span><span leaf="">.aggr_hidden_method)</span><br></code></pre><h5 data-line="211"><span leaf="">基于前面定义的采样和节点特征更新方式，就可以实现计算节点嵌入的方法。下面定义了一个两层的模型，隐藏层节点数为64，假设每阶采样点数都为10，那么计算中心节点的输出可以通过以下代码实现。其中前向传播时传入的参数node_features_list是一个列表，其中第0个元素代表源节点的特征，其后的元素表示每阶采样得到的节点特征。</span></h5><pre><code><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">__init__</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">, input_dim, hidden_dim,</span><br><span leaf="">                 num_neighbors_list)</span></span></span><span leaf="">:</span><br><span leaf="">        </span><span><span leaf="">super</span></span><span leaf="">(GraphSage, </span><span><span leaf="">self</span></span><span leaf="">).__init_</span><span><span leaf="">_</span></span><span leaf="">()</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.input_dim = input_dim</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.hidden_dim = hidden_dim</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.num_neighbors_list = num_neighbors_list</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.num_layers = len(num_neighbors_list)</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.gcn = nn.ModuleList()</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.gcn.append(SageGCN(input_dim, hidden_dim[</span><span><span leaf="">0</span></span><span leaf="">]))</span><br><span leaf="">        </span><span><span leaf="">for</span></span><span leaf=""> index </span><span><span leaf="">in</span></span><span leaf=""> range(</span><span><span leaf="">0</span></span><span leaf="">, len(hidden_dim) - </span><span><span leaf="">2</span></span><span leaf="">):</span><br><span leaf="">            </span><span><span leaf="">self</span></span><span leaf="">.gcn.append(SageGCN(hidden_dim[index], hidden_dim[index+</span><span><span leaf="">1</span></span><span leaf="">]))</span><br><span leaf="">        </span><span><span leaf="">self</span></span><span leaf="">.gcn.append(SageGCN(hidden_dim[-</span><span><span leaf="">2</span></span><span leaf="">], hidden_dim[-</span><span><span leaf="">1</span></span><span leaf="">], activation=None))</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">forward</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">, node_features_list)</span></span></span><span leaf="">:</span><br><span leaf="">        hidden = node_features_list</span><br><span leaf="">        </span><span><span leaf="">for</span></span><span leaf=""> l </span><span><span leaf="">in</span></span><span leaf=""> range(</span><span><span leaf="">self</span></span><span leaf="">.num_layers):</span><br><span leaf="">            next_hidden = []</span><br><span leaf="">            gcn = </span><span><span leaf="">self</span></span><span leaf="">.gcn[l]</span><br><span leaf="">            </span><span><span leaf="">for</span></span><span leaf=""> hop </span><span><span leaf="">in</span></span><span leaf=""> range(</span><span><span leaf="">self</span></span><span leaf="">.num_layers - l):</span><br><span leaf="">                src_node_features = hidden[hop]</span><br><span leaf="">                src_node_num = len(src_node_features)</span><br><span leaf="">                neighbor_node_features = hidden[hop + </span><span><span leaf="">1</span></span><span leaf="">] \</span><br><span leaf="">                    .view((src_node_num, </span><span><span leaf="">self</span></span><span leaf="">.num_neighbors_list[hop], -</span><span><span leaf="">1</span></span><span leaf="">))</span><br><span leaf="">                h = gcn(src_node_features, neighbor_node_features)</span><br><span leaf="">                next_hidden.append(h)</span><br><span leaf="">            hidden = next_hidden</span><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> hidden[</span><span><span leaf="">0</span></span><span leaf="">]</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">extra_repr</span></span><span><span leaf="">(</span><span><span leaf="">self</span></span><span leaf="">)</span></span></span><span leaf="">:</span><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> </span><span><span leaf="">'in_features={}, num_neighbors_list={}'</span></span><span leaf="">.format(</span><br><span leaf="">            </span><span><span leaf="">self</span></span><span leaf="">.input_dim, </span><span><span leaf="">self</span></span><span leaf="">.num_neighbors_list</span><br><span leaf="">        )</span><br></code></pre><h4 data-line="246"><span leaf="">4、数据处理（data.py）</span></h4><pre><code><span><span leaf="">import</span></span><span leaf=""> os</span><br><span><span leaf="">import</span></span><span leaf=""> os.path </span><span><span leaf="">as</span></span><span leaf=""> osp</span><br><span><span leaf="">import</span></span><span leaf=""> pickle</span><br><span><span leaf="">import</span></span><span leaf=""> itertools</span><br><span><span leaf="">import</span></span><span leaf=""> scipy.sparse </span><span><span leaf="">as</span></span><span leaf=""> sp</span><br><span><span leaf="">import</span></span><span leaf=""> urllib</span><br><span><span leaf="">from</span></span><span leaf=""> collections </span><span><span leaf="">import</span></span><span leaf=""> namedtuple</span><br><span><span leaf="">import</span></span><span leaf=""> numpy </span><span><span leaf="">as</span></span><span leaf=""> np</span><br><br><span leaf="">Data = namedtuple(</span><span><span leaf="">'Data'</span></span><span leaf="">, [</span><span><span leaf="">'x'</span></span><span leaf="">, </span><span><span leaf="">'y'</span></span><span leaf="">, </span><span><span leaf="">'adjacency_dict'</span></span><span leaf="">,</span><br><span leaf="">                           </span><span><span leaf="">'train_mask'</span></span><span leaf="">, </span><span><span leaf="">'val_mask'</span></span><span leaf="">, </span><span><span leaf="">'test_mask'</span></span><span leaf="">])</span><br><br><span><span><span leaf="">class</span></span><span leaf=""> </span><span><span leaf="">CoraData</span></span><span><span leaf="">(object)</span></span><span leaf="">:</span></span><br><span leaf="">    download_url = </span><span><span leaf="">"https://github.com/kimiyoung/planetoid/raw/master/data"</span></span><br><span leaf="">    filenames = [</span><span><span leaf="">"ind.cora.{}"</span></span><span leaf="">.format(name) </span><span><span leaf="">for</span></span><span leaf=""> name </span><span><span leaf="">in</span></span><br><span leaf="">                 [</span><span><span leaf="">'x'</span></span><span leaf="">, </span><span><span leaf="">'tx'</span></span><span leaf="">, </span><span><span leaf="">'allx'</span></span><span leaf="">, </span><span><span leaf="">'y'</span></span><span leaf="">, </span><span><span leaf="">'ty'</span></span><span leaf="">, </span><span><span leaf="">'ally'</span></span><span leaf="">, </span><span><span leaf="">'graph'</span></span><span leaf="">, </span><span><span leaf="">'test.index'</span></span><span leaf="">]]</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">__init__</span></span><span><span leaf="">(self, data_root=</span><span><span leaf="">"cora"</span></span><span leaf="">, rebuild=False)</span></span><span leaf="">:</span></span><br><span leaf="">        </span><span><span leaf="">"""Cora数据，包括数据下载，处理，加载等功能</span><br><span leaf="">        当数据的缓存文件存在时，将使用缓存文件，否则将下载、进行处理，并缓存到磁盘</span><br><span leaf="">        处理之后的数据可以通过属性 .data 获得，它将返回一个数据对象，包括如下几部分：</span><br><span leaf="">            * x: 节点的特征，维度为 2708 * 1433，类型为 np.ndarray</span><br><span leaf="">            * y: 节点的标签，总共包括7个类别，类型为 np.ndarray</span><br><span leaf="">            * adjacency_dict: 邻接信息，，类型为 dict</span><br><span leaf="">            * train_mask: 训练集掩码向量，维度为 2708，当节点属于训练集时，相应位置为True，否则False</span><br><span leaf="">            * val_mask: 验证集掩码向量，维度为 2708，当节点属于验证集时，相应位置为True，否则False</span><br><span leaf="">            * test_mask: 测试集掩码向量，维度为 2708，当节点属于测试集时，相应位置为True，否则False</span><br><span leaf="">        Args:</span><br><span leaf="">        -------</span><br><span leaf="">            data_root: string, optional</span><br><span leaf="">                存放数据的目录，原始数据路径: {data_root}/raw</span><br><span leaf="">                缓存数据路径: {data_root}/processed_cora.pkl</span><br><span leaf="">            rebuild: boolean, optional</span><br><span leaf="">                是否需要重新构建数据集，当设为True时，如果存在缓存数据也会重建数据</span><br><span leaf="">        """</span></span><br><span leaf="">        self.data_root = data_root</span><br><span leaf="">        save_file = osp.join(self.data_root, </span><span><span leaf="">"processed_cora.pkl"</span></span><span leaf="">)</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> osp.exists(save_file) </span><span><span leaf="">and</span></span><span leaf=""> </span><span><span leaf="">not</span></span><span leaf=""> rebuild:</span><br><span leaf="">            print(</span><span><span leaf="">"Using Cached file: {}"</span></span><span leaf="">.format(save_file))</span><br><span leaf="">            self._data = pickle.load(open(save_file, </span><span><span leaf="">"rb"</span></span><span leaf="">))</span><br><span leaf="">        </span><span><span leaf="">else</span></span><span leaf="">:</span><br><span leaf="">            self.maybe_download()</span><br><span leaf="">            self._data = self.process_data()</span><br><span leaf="">            </span><span><span leaf="">with</span></span><span leaf=""> open(save_file, </span><span><span leaf="">"wb"</span></span><span leaf="">) </span><span><span leaf="">as</span></span><span leaf=""> f:</span><br><span leaf="">                pickle.dump(self.data, f)</span><br><span leaf="">            print(</span><span><span leaf="">"Cached file: {}"</span></span><span leaf="">.format(save_file))</span><br><br><span><span leaf="">    @property</span></span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">data</span></span><span><span leaf="">(self)</span></span><span leaf="">:</span></span><br><span leaf="">        </span><span><span leaf="">"""返回Data数据对象，包括x, y, adjacency, train_mask, val_mask, test_mask"""</span></span><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> self._data</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">process_data</span></span><span><span leaf="">(self)</span></span><span leaf="">:</span></span><br><span leaf="">        </span><span><span leaf="">"""</span><br><span leaf="">        处理数据，得到节点特征和标签，邻接矩阵，训练集、验证集以及测试集</span><br><span leaf="">        引用自：https://github.com/rusty1s/pytorch_geometric</span><br><span leaf="">        """</span></span><br><span leaf="">        print(</span><span><span leaf="">"Process data ..."</span></span><span leaf="">)</span><br><span leaf="">        _, tx, allx, y, ty, ally, graph, test_index = [self.read_data(</span><br><span leaf="">            osp.join(self.data_root, </span><span><span leaf="">"raw"</span></span><span leaf="">, name)) </span><span><span leaf="">for</span></span><span leaf=""> name </span><span><span leaf="">in</span></span><span leaf=""> self.filenames]</span><br><span leaf="">        train_index = np.arange(y.shape[</span><span><span leaf="">0</span></span><span leaf="">])</span><br><span leaf="">        val_index = np.arange(y.shape[</span><span><span leaf="">0</span></span><span leaf="">], y.shape[</span><span><span leaf="">0</span></span><span leaf="">] + </span><span><span leaf="">500</span></span><span leaf="">)</span><br><span leaf="">        sorted_test_index = sorted(test_index)</span><br><br><span leaf="">        x = np.concatenate((allx, tx), axis=</span><span><span leaf="">0</span></span><span leaf="">)</span><br><span leaf="">        y = np.concatenate((ally, ty), axis=</span><span><span leaf="">0</span></span><span leaf="">).argmax(axis=</span><span><span leaf="">1</span></span><span leaf="">)</span><br><br><span leaf="">        x[test_index] = x[sorted_test_index]</span><br><span leaf="">        y[test_index] = y[sorted_test_index]</span><br><span leaf="">        num_nodes = x.shape[</span><span><span leaf="">0</span></span><span leaf="">]</span><br><br><span leaf="">        train_mask = np.zeros(num_nodes, dtype=np.bool)</span><br><span leaf="">        val_mask = np.zeros(num_nodes, dtype=np.bool)</span><br><span leaf="">        test_mask = np.zeros(num_nodes, dtype=np.bool)</span><br><span leaf="">        train_mask[train_index] = </span><span><span leaf="">True</span></span><br><span leaf="">        val_mask[val_index] = </span><span><span leaf="">True</span></span><br><span leaf="">        test_mask[test_index] = </span><span><span leaf="">True</span></span><br><span leaf="">        adjacency_dict = graph</span><br><span leaf="">        print(</span><span><span leaf="">"Node's feature shape: "</span></span><span leaf="">, x.shape)</span><br><span leaf="">        print(</span><span><span leaf="">"Node's label shape: "</span></span><span leaf="">, y.shape)</span><br><span leaf="">        print(</span><span><span leaf="">"Adjacency's shape: "</span></span><span leaf="">, len(adjacency_dict))</span><br><span leaf="">        print(</span><span><span leaf="">"Number of training nodes: "</span></span><span leaf="">, train_mask.sum())</span><br><span leaf="">        print(</span><span><span leaf="">"Number of validation nodes: "</span></span><span leaf="">, val_mask.sum())</span><br><span leaf="">        print(</span><span><span leaf="">"Number of test nodes: "</span></span><span leaf="">, test_mask.sum())</span><br><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> Data(x=x, y=y, adjacency_dict=adjacency_dict,</span><br><span leaf="">                    train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)</span><br><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">maybe_download</span></span><span><span leaf="">(self)</span></span><span leaf="">:</span></span><br><span leaf="">        save_path = os.path.join(self.data_root, </span><span><span leaf="">"raw"</span></span><span leaf="">)</span><br><span leaf="">        </span><span><span leaf="">for</span></span><span leaf=""> name </span><span><span leaf="">in</span></span><span leaf=""> self.filenames:</span><br><span leaf="">            </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">not</span></span><span leaf=""> osp.exists(osp.join(save_path, name)):</span><br><span leaf="">                self.download_data(</span><br><span leaf="">                    </span><span><span leaf="">"{}/{}"</span></span><span leaf="">.format(self.download_url, name), save_path)</span><br><br><span><span leaf="">    @staticmethod</span></span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">build_adjacency</span></span><span><span leaf="">(adj_dict)</span></span><span leaf="">:</span></span><br><span leaf="">        </span><span><span leaf="">"""根据邻接表创建邻接矩阵"""</span></span><br><span leaf="">        edge_index = []</span><br><span leaf="">        num_nodes = len(adj_dict)</span><br><span leaf="">        </span><span><span leaf="">for</span></span><span leaf=""> src, dst </span><span><span leaf="">in</span></span><span leaf=""> adj_dict.items():</span><br><span leaf="">            edge_index.extend([src, v] </span><span><span leaf="">for</span></span><span leaf=""> v </span><span><span leaf="">in</span></span><span leaf=""> dst)</span><br><span leaf="">            edge_index.extend([v, src] </span><span><span leaf="">for</span></span><span leaf=""> v </span><span><span leaf="">in</span></span><span leaf=""> dst)</span><br><span leaf="">        </span><span><span leaf=""># 去除重复的边</span></span><br><span leaf="">        edge_index = list(k </span><span><span leaf="">for</span></span><span leaf=""> k, _ </span><span><span leaf="">in</span></span><span leaf=""> itertools.groupby(sorted(edge_index)))</span><br><span leaf="">        edge_index = np.asarray(edge_index)</span><br><span leaf="">        adjacency = sp.coo_matrix((np.ones(len(edge_index)),</span><br><span leaf="">                                   (edge_index[:, </span><span><span leaf="">0</span></span><span leaf="">], edge_index[:, </span><span><span leaf="">1</span></span><span leaf="">])),</span><br><span leaf="">                                  shape=(num_nodes, num_nodes), dtype=</span><span><span leaf="">"float32"</span></span><span leaf="">)</span><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> adjacency</span><br><br><span><span leaf="">    @staticmethod</span></span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">read_data</span></span><span><span leaf="">(path)</span></span><span leaf="">:</span></span><br><span leaf="">        </span><span><span leaf="">"""使用不同的方式读取原始数据以进一步处理"""</span></span><br><span leaf="">        name = osp.basename(path)</span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> name == </span><span><span leaf="">"ind.cora.test.index"</span></span><span leaf="">:</span><br><span leaf="">            out = np.genfromtxt(path, dtype=</span><span><span leaf="">"int64"</span></span><span leaf="">)</span><br><span leaf="">            </span><span><span leaf="">return</span></span><span leaf=""> out</span><br><span leaf="">        </span><span><span leaf="">else</span></span><span leaf="">:</span><br><span leaf="">            out = pickle.load(open(path, </span><span><span leaf="">"rb"</span></span><span leaf="">), encoding=</span><span><span leaf="">"latin1"</span></span><span leaf="">)</span><br><span leaf="">            out = out.toarray() </span><span><span leaf="">if</span></span><span leaf=""> hasattr(out, </span><span><span leaf="">"toarray"</span></span><span leaf="">) </span><span><span leaf="">else</span></span><span leaf=""> out</span><br><span leaf="">            </span><span><span leaf="">return</span></span><span leaf=""> out</span><br><br><span><span leaf="">    @staticmethod</span></span><br><span leaf="">    </span><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">download_data</span></span><span><span leaf="">(url, save_path)</span></span><span leaf="">:</span></span><br><span leaf="">        </span><span><span leaf="">"""数据下载工具，当原始数据不存在时将会进行下载"""</span></span><br><span leaf="">        </span><span><span leaf="">if</span></span><span leaf=""> </span><span><span leaf="">not</span></span><span leaf=""> os.path.exists(save_path):</span><br><span leaf="">            os.makedirs(save_path)</span><br><span leaf="">        data = urllib.request.urlopen(url)</span><br><span leaf="">        filename = os.path.split(url)[</span><span><span leaf="">-1</span></span><span leaf="">]</span><br><br><span leaf="">        </span><span><span leaf="">with</span></span><span leaf=""> open(os.path.join(save_path, filename), </span><span><span leaf="">'wb'</span></span><span leaf="">) </span><span><span leaf="">as</span></span><span leaf=""> f:</span><br><span leaf="">            f.write(data.read())</span><br><br><span leaf="">        </span><span><span leaf="">return</span></span><span leaf=""> </span><span><span leaf="">True</span></span><br></code></pre><h4 data-line="385"><span leaf="">主脚本</span></h4><pre><code><span><span leaf="">import</span></span><span leaf=""> torch</span><br><br><span><span leaf="">import</span></span><span leaf=""> numpy </span><span><span leaf="">as</span></span><span leaf=""> np</span><br><span><span leaf="">import</span></span><span leaf=""> torch.nn </span><span><span leaf="">as</span></span><span leaf=""> nn</span><br><span><span leaf="">import</span></span><span leaf=""> torch.optim </span><span><span leaf="">as</span></span><span leaf=""> optim</span><br><span><span leaf="">from</span></span><span leaf=""> net </span><span><span leaf="">import</span></span><span leaf=""> GraphSage</span><br><span><span leaf="">from</span></span><span leaf=""> data </span><span><span leaf="">import</span></span><span leaf=""> CoraData</span><br><span><span leaf="">from</span></span><span leaf=""> sampling </span><span><span leaf="">import</span></span><span leaf=""> multihop_sampling</span><br><br><span><span leaf="">from</span></span><span leaf=""> collections </span><span><span leaf="">import</span></span><span leaf=""> namedtuple</span><br><br><span><span leaf="">###数据准备</span></span><br><span leaf="">Data = namedtuple(</span><span><span leaf="">'Data'</span></span><span leaf="">, [</span><span><span leaf="">'x'</span></span><span leaf="">, </span><span><span leaf="">'y'</span></span><span leaf="">, </span><span><span leaf="">'adjacency_dict'</span></span><span leaf="">,</span><br><span leaf="">                           </span><span><span leaf="">'train_mask'</span></span><span leaf="">, </span><span><span leaf="">'val_mask'</span></span><span leaf="">, </span><span><span leaf="">'test_mask'</span></span><span leaf="">])</span><br><br><span leaf="">data = CoraData().data</span><br><span leaf="">x = data.x / data.x.sum(</span><span><span leaf="">1</span></span><span leaf="">, keepdims=</span><span><span leaf="">True</span></span><span leaf="">)  </span><span><span leaf=""># 归一化数据，使得每一行和为1</span></span><br><br><span leaf="">train_index = np.where(data.train_mask)[</span><span><span leaf="">0</span></span><span leaf="">]</span><br><span leaf="">train_label = data.y[train_index]</span><br><span leaf="">test_index = np.where(data.test_mask)[</span><span><span leaf="">0</span></span><span leaf="">]</span><br><br><span><span leaf="">####模型初始化</span></span><br><span leaf="">INPUT_DIM = </span><span><span leaf="">1433</span></span><span leaf="">    </span><span><span leaf=""># 输入维度</span></span><br><span><span leaf=""># Note: 采样的邻居阶数需要与GCN的层数保持一致</span></span><br><span leaf="">HIDDEN_DIM = [</span><span><span leaf="">128</span></span><span leaf="">, </span><span><span leaf="">7</span></span><span leaf="">]   </span><span><span leaf=""># 隐藏单元节点数</span></span><br><span leaf="">NUM_NEIGHBORS_LIST = [</span><span><span leaf="">10</span></span><span leaf="">, </span><span><span leaf="">10</span></span><span leaf="">]   </span><span><span leaf=""># 每阶采样邻居的节点数</span></span><br><span><span leaf="">assert</span></span><span leaf=""> len(HIDDEN_DIM) == len(NUM_NEIGHBORS_LIST)</span><br><span leaf="">BTACH_SIZE = </span><span><span leaf="">16</span></span><span leaf="">     </span><span><span leaf=""># 批处理大小</span></span><br><span leaf="">EPOCHS = </span><span><span leaf="">20</span></span><br><span leaf="">NUM_BATCH_PER_EPOCH = </span><span><span leaf="">20</span></span><span leaf="">    </span><span><span leaf=""># 每个epoch循环的批次数</span></span><br><span leaf="">LEARNING_RATE = </span><span><span leaf="">0.01</span></span><span leaf="">    </span><span><span leaf=""># 学习率</span></span><br><span leaf="">DEVICE = </span><span><span leaf="">"cuda"</span></span><span leaf=""> </span><span><span leaf="">if</span></span><span leaf=""> torch.cuda.is_available() </span><span><span leaf="">else</span></span><span leaf=""> </span><span><span leaf="">"cpu"</span></span><br><br><span leaf="">model = GraphSage(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,</span><br><span leaf="">                  num_neighbors_list=NUM_NEIGHBORS_LIST).to(DEVICE)</span><br><span leaf="">print(model)</span><br><span leaf="">criterion = nn.CrossEntropyLoss().to(DEVICE)</span><br><span leaf="">optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=</span><span><span leaf="">5e-4</span></span><span leaf="">)</span><br><br><span><span leaf="">####模型训练和测试</span></span><br><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">train</span></span><span><span leaf="">()</span></span><span leaf="">:</span></span><br><span leaf="">    model.train()</span><br><span leaf="">    </span><span><span leaf="">for</span></span><span leaf=""> e </span><span><span leaf="">in</span></span><span leaf=""> range(EPOCHS):</span><br><span leaf="">        </span><span><span leaf="">for</span></span><span leaf=""> batch </span><span><span leaf="">in</span></span><span leaf=""> range(NUM_BATCH_PER_EPOCH):</span><br><span leaf="">            batch_src_index = np.random.choice(train_index, size=(BTACH_SIZE,))</span><br><span leaf="">            batch_src_label = torch.from_numpy(train_label[batch_src_index]).long().to(DEVICE)</span><br><span leaf="">            batch_sampling_result = multihop_sampling(batch_src_index, NUM_NEIGHBORS_LIST, data.adjacency_dict)</span><br><span leaf="">            batch_sampling_x = [torch.from_numpy(x[idx]).float().to(DEVICE) </span><span><span leaf="">for</span></span><span leaf=""> idx </span><span><span leaf="">in</span></span><span leaf=""> batch_sampling_result]</span><br><span leaf="">            batch_train_logits = model(batch_sampling_x)</span><br><span leaf="">            loss = criterion(batch_train_logits, batch_src_label)</span><br><span leaf="">            optimizer.zero_grad()</span><br><span leaf="">            loss.backward()  </span><span><span leaf=""># 反向传播计算参数的梯度</span></span><br><span leaf="">            optimizer.step()  </span><span><span leaf=""># 使用优化方法进行梯度更新</span></span><br><span leaf="">            print(</span><span><span leaf="">"Epoch {:03d} Batch {:03d} Loss: {:.4f}"</span></span><span leaf="">.format(e, batch, loss.item()))</span><br><span leaf="">        test()</span><br><br><br><span><span><span leaf="">def</span></span><span leaf=""> </span><span><span leaf="">test</span></span><span><span leaf="">()</span></span><span leaf="">:</span></span><br><span leaf="">    model.eval()</span><br><span leaf="">    </span><span><span leaf="">with</span></span><span leaf=""> torch.no_grad():</span><br><span leaf="">        test_sampling_result = multihop_sampling(test_index, NUM_NEIGHBORS_LIST, data.adjacency_dict)</span><br><span leaf="">        test_x = [torch.from_numpy(x[idx]).float().to(DEVICE) </span><span><span leaf="">for</span></span><span leaf=""> idx </span><span><span leaf="">in</span></span><span leaf=""> test_sampling_result]</span><br><span leaf="">        test_logits = model(test_x)</span><br><span leaf="">        test_label = torch.from_numpy(data.y[test_index]).long().to(DEVICE)</span><br><span leaf="">        predict_y = test_logits.max(</span><span><span leaf="">1</span></span><span leaf="">)[</span><span><span leaf="">1</span></span><span leaf="">]</span><br><span leaf="">        accuarcy = torch.eq(predict_y, test_label).float().mean().item()</span><br><span leaf="">        print(</span><span><span leaf="">"Test Accuracy: "</span></span><span leaf="">, accuarcy)</span><br><br><span leaf="">train()</span><br></code></pre><h4 data-line="459"><span leaf="">当然了，方法有很多，我们这里仅是冰山一角。</span></h4><h4 data-line="461"><span leaf="">生活很好，有你更好</span></h4><section nodeleaf=""><mp-common-cpsad data-pluginname="mpcps" data-templateid="list" data-cpsversion="v120" data-goodssouce="1" data-traceid="d5432d83-74b9-4b56-a52b-0880b2c42b40" data-pid="101_10047796968606"></mp-common-cpsad></section><section><span leaf=""><br></span></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/qjD5H0v_4Ztz3q99Cfj6ug",target="_blank" rel="noopener noreferrer">原文链接</a>
