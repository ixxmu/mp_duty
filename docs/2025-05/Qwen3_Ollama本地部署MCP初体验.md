---
title: "Qwen3+Ollama本地部署MCP初体验"
date: 2025-05-03T10:22:41Z
draft: ["false"]
tags: [
  "fetched",
  "PyTorch研习社"
]
categories: ["Acdemic"]
---
Qwen3+Ollama本地部署MCP初体验 by PyTorch研习社
------
<div><blockquote><h3><span leaf="">前言</span></h3></blockquote><p><span leaf="">小伙伴们五一快乐鸭，Qwen3已经发布好几天，热度依然不减；五一期间笔者尝试在本地体验Qwen3的能力，将所有过程分享给大家，大家一起动手试一试，一起玩转Qwen3;</span></p><p><span leaf="">当然除了本地这种模式，也可以选择云端的API，这里就不赘述了。</span></p><section nodeleaf=""><img data-imgfileid="100000424" data-ratio="0.425531914893617" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGZJQaLYWI0hSj37C6BLWb96rsfmseibE5mYkPiaDxPDVNZTperuiaklqZg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="2350" type="block" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGZJQaLYWI0hSj37C6BLWb96rsfmseibE5mYkPiaDxPDVNZTperuiaklqZg/640?wx_fmt=png&amp;from=appmsg"></section><p><span leaf=""><br></span></p><section nodeleaf=""><mp-common-profile data-pluginname="mpprofile" data-nickname="AI大模型观察站" data-from="0" data-headimg="http://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTwibA7LuKNnOHKXDgpHTfxaPLDO3ic0jSxUc4nPEWzLddkwp9HsI096p3SmlauWP5nGYTDtDK5cMVzg/0?wx_fmt=png" data-signature="专注于人工智能大模型的最新进展，涵盖Transformer架构、LLM训练优化、推理加速、多模态应用等核心技术领域。通过深度解析论文、开源项目和行业动态，揭示大模型技术的演进趋势，助力开发者、研究者和AI爱好者把握前沿创新。" data-id="MzkzMjkwMjk3Mw==" data-is_biz_ban="0" data-service_type="1" data-verify_status="0"></mp-common-profile></section><p><span leaf=""><br></span></p><hr><blockquote><h3><span leaf="">知识点</span></h3></blockquote><p><span leaf="">通过本文大家需要掌握以下知识点</span></p><ul><li><span><span><span leaf="">Qwen3 相关知识</span></span></span></li><li><span><span><span leaf="">Ollama</span></span></span></li><li><span><span><span leaf="">Open WebUI (一个和ChatGPT类似的聊天的Web页面,支持ollama)</span></span></span></li><li><span><span><span leaf="">MCP 相关知识</span></span></span></li></ul><p><span leaf="">MCP相关知识可以看这里： xxxx</span></p><hr><blockquote><h3><span leaf="">Qwen3</span></h3></blockquote><p><span leaf="">阿里推出 Qwen3，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型 Qwen3-235B-A22B 在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 Qwen3-30B-A3B 的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。</span></p><p><span leaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGDRAVmf3oaCx0BAyRVClUibtJ7Lo8nRen3SzgZcOZ35ibS27jiaWTvwfKw/640?wx_fmt=jpeg&amp;from=appmsg" data-ratio="0.562962962962963" data-type="jpeg" data-w="1080" data-imgfileid="100000427" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGDRAVmf3oaCx0BAyRVClUibtJ7Lo8nRen3SzgZcOZ35ibS27jiaWTvwfKw/640?wx_fmt=jpeg&amp;from=appmsg"></span></p><p><span leaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGbKDcMMPPpPesEMqP9XlohwfZCYFQmEoVaX02gJibyc5uf3wzQWmMF7Q/640?wx_fmt=jpeg&amp;from=appmsg" data-ratio="0.562962962962963" data-type="jpeg" data-w="1080" data-imgfileid="100000428" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGbKDcMMPPpPesEMqP9XlohwfZCYFQmEoVaX02gJibyc5uf3wzQWmMF7Q/640?wx_fmt=jpeg&amp;from=appmsg"></span></p><p><span leaf="">开源了两个 MoE 模型的权重：Qwen3-235B-A22B，一个拥有 2350 多亿总参数和 220 多亿激活参数的大模型，以及Qwen3-30B-A3B，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense 模型也已开源，包括 Qwen3-32B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B 和 Qwen3-0.6B，均在 Apache 2.0 许可下开源。</span></p><table><thead><tr><th><section><span leaf="">Models</span></section></th><th><section><span leaf="">Layers</span></section></th><th><section><span leaf="">Heads (Q / KV)</span></section></th><th><section><span leaf="">Tie Embedding</span></section></th><th><section><span leaf="">Context Length</span></section></th></tr></thead><tbody><tr><td><section><span leaf="">Qwen3-0.6B</span></section></td><td><section><span leaf="">28</span></section></td><td><section><span leaf="">16 / 8</span></section></td><td><section><span leaf="">Yes</span></section></td><td><section><span leaf="">32K</span></section></td></tr><tr><td><section><span leaf="">Qwen3-1.7B</span></section></td><td><section><span leaf="">28</span></section></td><td><section><span leaf="">16 / 8</span></section></td><td><section><span leaf="">Yes</span></section></td><td><section><span leaf="">32K</span></section></td></tr><tr><td><section><span leaf="">Qwen3-4B</span></section></td><td><section><span leaf="">36</span></section></td><td><section><span leaf="">32 / 8</span></section></td><td><section><span leaf="">Yes</span></section></td><td><section><span leaf="">32K</span></section></td></tr><tr><td><section><span leaf="">Qwen3-8B</span></section></td><td><section><span leaf="">36</span></section></td><td><section><span leaf="">32 / 8</span></section></td><td><section><span leaf="">No</span></section></td><td><section><span leaf="">128K</span></section></td></tr><tr><td><section><span leaf="">Qwen3-14B</span></section></td><td><section><span leaf="">40</span></section></td><td><section><span leaf="">40 / 8</span></section></td><td><section><span leaf="">No</span></section></td><td><section><span leaf="">128K</span></section></td></tr><tr><td><section><span leaf="">Qwen3-32B</span></section></td><td><section><span leaf="">64</span></section></td><td><section><span leaf="">64 / 8</span></section></td><td><section><span leaf="">No</span></section></td><td><section><span leaf="">128K</span></section></td></tr></tbody></table><hr><table><thead><tr><th><section><span leaf="">Models</span></section></th><th><section><span leaf="">Layers</span></section></th><th><section><span leaf="">Heads (Q / KV)</span></section></th><th><section><span leaf=""># Experts (Total / Activated)</span></section></th><th><section><span leaf="">Context Length</span></section></th></tr></thead><tbody><tr><td><section><span leaf="">Qwen3-30B-A3B</span></section></td><td><section><span leaf="">48</span></section></td><td><section><span leaf="">32 / 4</span></section></td><td><section><span leaf="">128 / 8</span></section></td><td><section><span leaf="">128K</span></section></td></tr><tr><td><section><span leaf="">Qwen3-235B-A22B</span></section></td><td><section><span leaf="">94</span></section></td><td><section><span leaf="">64 / 4</span></section></td><td><section><span leaf="">128 / 8</span></section></td><td><section><span leaf="">28K</span></section></td></tr></tbody></table><p><span leaf="">经过后训练的模型，例如 Qwen3-30B-A3B，以及它们的预训练基座模型（如 Qwen3-30B-A3B-Base），现已在 Hugging Face、ModelScope 和 Kaggle 等平台上开放使用。对于部署，我们推荐使用 SGLang 和 vLLM 等框架；而对于本地使用，像 </span><strong><span leaf="">Ollama</span></strong><span leaf="">、LMStudio、MLX、llama.cpp 和 KTransformers 这样的工具也非常值得推荐。这些选项确保用户可以轻松将 Qwen3 集成到他们的工作流程中，无论是用于研究、开发还是生产环境。</span></p><p><span leaf="">官网地址：https://qwenlm.github.io/zh/blog/qwen3/ 下文将使用Ollama进行Qwen3本地化</span></p><hr><blockquote><h3><span leaf="">Ollama</span></h3></blockquote><p><span leaf="">Ollama 是一个轻量级、用户友好的框架，旨在让用户在本地运行开源大型语言模型，如 Llama 3、DeepSeek-R1、Gemma、Mistral、Qwen 等。它通过 Modelfile 将模型权重、配置和数据打包成一个统一包，类似于 Docker 镜像的概念，优化了模型的设置和 GPU 使用。Ollama 的核心优势在于隐私性、灵活性和离线可用性，特别适合对数据安全有高要求的场景。</span></p><section nodeleaf=""><mp-common-profile data-pluginname="mpprofile" data-nickname="AI大模型观察站" data-from="0" data-headimg="http://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTwibA7LuKNnOHKXDgpHTfxaPLDO3ic0jSxUc4nPEWzLddkwp9HsI096p3SmlauWP5nGYTDtDK5cMVzg/0?wx_fmt=png" data-signature="专注于人工智能大模型的最新进展，涵盖Transformer架构、LLM训练优化、推理加速、多模态应用等核心技术领域。通过深度解析论文、开源项目和行业动态，揭示大模型技术的演进趋势，助力开发者、研究者和AI爱好者把握前沿创新。" data-id="MzkzMjkwMjk3Mw==" data-is_biz_ban="0" data-service_type="1" data-verify_status="0"></mp-common-profile></section><p><span leaf=""><br></span></p><h4><span leaf="">主要特点</span></h4><p><strong><span leaf="">本地运行：</span></strong></p><ul><li><span><span><span leaf="">所有数据处理都在本地进行，无需将敏感数据发送到云端，保障隐私和安全。</span></span></span></li><li><span><span><span leaf="">支持离线使用，无需互联网连接即可运行模型，适合无网络环境下的应用。</span></span></span></li></ul><p><strong><span leaf="">广泛的模型支持：</span></strong></p><ul><li><span><span><span leaf="">支持多种开源模型，包括 Llama 3.2、Gemma 2、Mistral、Codestral 等，适用于文本生成、代码生成、翻译等任务。</span></span></span></li><li><span><span><span leaf="">用户可从官方模型库拉取预训练模型，或通过 Modelfile 创建自定义模型。</span></span></span></li></ul><p><strong><span leaf="">跨平台兼容性：</span></strong></p><ul><li><span><span><span leaf="">支持 macOS、Linux 和 Windows（Windows 支持为预览版）。</span></span></span></li><li><span><span><span leaf="">可在本地设备或虚拟专用服务器（VPS）上运行，适合个人项目或团队协作。</span></span></span></li></ul><p><strong><span leaf="">用户友好的接口：</span></strong></p><ul><li><span><span><span leaf="">主要通过命令行界面（CLI）操作，适合技术用户快速拉取、运行和管理模型。</span></span></span></li><li><span><span><span leaf="">支持第三方图形用户界面（如 Open WebUI），提供更直观的操作体验。</span></span></span></li></ul><p><strong><span leaf="">API 集成：</span></strong></p><ul><li><span><span><span leaf="">提供 REST API，支持 Python、JavaScript 等编程语言集成。</span></span></span></li><li><span><span><span leaf="">与 LangChain、LlamaIndex 等框架无缝集成，方便构建复杂 AI 应用。</span></span></span></li></ul><h4><span leaf="">安装和使用步骤</span></h4><p><strong><span leaf="">下载和安装：</span></strong><span leaf="">访问 Ollama 官网 或 GitHub 页面，下载适用于您操作系统的安装包。 Linux 和 macOS 用户可通过以下命令快速安装：</span></p><pre><ol><li><span><span><code><span><span leaf="">curl </span></span><span><span leaf="">-</span></span><span><span leaf="">fsSL https</span></span><span><span leaf="">:</span></span><span><span leaf="">//ollama.com/install.sh | sh</span></span></code></span></span></li></ol></pre><p><span leaf="">Windows 用户需下载安装程序并按提示操作。</span></p><p><strong><span leaf="">拉取模型：</span></strong></p><p><span leaf="">使用 CLI 拉取所需模型，例如：</span></p><pre><ol><li><span><span><code><span><span leaf="">ollama pull llama3</span></span><span><span leaf="">.</span></span><span><span leaf="">2</span></span></code></span></span></li></ol></pre><p><span leaf="">可从 Ollama 模型库 查看可用模型。</span></p><p><strong><span leaf="">运行模型：</span></strong></p><p><span leaf="">通过命令运行模型并直接交互：</span></p><pre><ol><li><span><span><code><span><span leaf="">ollama run llama3</span></span><span><span leaf="">.</span></span><span><span leaf="">2</span></span></code></span></span></li></ol></pre><p><span leaf="">输入提示（如“解释机器学习基础”），模型将生成响应。</span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGfeqtaiawS7mD9YxNnm2FdeE3m2FNonckiaM8IibuIOfS2HNibWLVwnwnibA/640?wx_fmt=jpeg&amp;from=appmsg" data-ratio="0.5805555555555556" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="100000429" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGfeqtaiawS7mD9YxNnm2FdeE3m2FNonckiaM8IibuIOfS2HNibWLVwnwnibA/640?wx_fmt=jpeg&amp;from=appmsg"></section><p><span leaf=""><br></span></p><p><strong><span leaf="">API 调用：</span></strong></p><p><span leaf="">启动 Ollama 服务器：</span></p><pre><ol><li><span><span><code><span><span leaf="">ollama serve</span></span></code></span></span></li></ol></pre><p><span leaf="">使用 cURL 或编程语言调用 API，例如：</span></p><pre><ol><li><span><span><code><span><span leaf="">curl http</span></span><span><span leaf="">:</span></span><span><span leaf="">//localhost:11434/api/generate -d '{"model": "llama3.2", "prompt": "Why is the sky blue?"}'</span></span></code></span></span></li></ol></pre><h4><span leaf="">相关资源</span></h4><p><span leaf="">官方网站：https://ollama.com/ GitHub 仓库：https://github.com/ollama/ollama 模型库：https://ollama.com/library 社区支持：Ollama 的 Discord 社区或 GitHub Issues 页面</span></p><p><span leaf="">Ollama 是一个强大的工具，通过简化本地运行大型语言模型的流程，为用户提供了隐私、安全和灵活的 AI 解决方案。无论您是想开发 AI 应用、进行研究，还是仅想探索开源模型的潜力，Ollama 都是一个值得尝试的平台。通过其 CLI、API 和丰富的模型支持，您可以轻松将尖端 AI 技术带到本地设备上。</span></p><hr><blockquote><h3><span leaf="">Ollama安装Qwen3</span></h3></blockquote><p><span leaf="">通过对ollama的学习，我们已经知道如何通过它安装模型，以下是安装Qwen3的命令</span></p><pre><ol><li><span><span><code><span><span leaf="">ollama run qwen3</span></span><span><span leaf="">:</span></span><span><span leaf="">14b</span></span></code></span></span></li></ol></pre><p><span leaf="">在ollama官网 Qwen3提供了</span></p><ul><li><span><span><span leaf="">0.6b</span></span></span></li><li><span><span><span leaf="">1.7b</span></span></span></li><li><span><span><span leaf="">4b</span></span></span></li><li><span><span><span leaf="">8b</span></span></span></li><li><span><span><span leaf="">14b</span></span></span></li><li><span><span><span leaf="">30b</span></span></span></li><li><span><span><span leaf="">32b</span></span></span></li><li><span><span><span leaf="">235b</span></span></span></li></ul><section><span leaf=""><br></span></section><section><span data-pm-slice="0 0 []"><span leaf="">笔者的运行情况如下</span></span></section><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGfCTOtyiaD6XmvRzuWac9uiakhV1PuJAibSreicMHID7ALhQbU8pSJBht6A/640?wx_fmt=png&amp;from=appmsg" data-ratio="0.3611111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000438" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGfCTOtyiaD6XmvRzuWac9uiakhV1PuJAibSreicMHID7ALhQbU8pSJBht6A/640?wx_fmt=png&amp;from=appmsg"></section><section><span data-pm-slice="0 0 []"><span leaf=""><br></span></span></section><p><span leaf="">当然你要运行其他模型可以在这里查找： https://ollama.com/search</span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGAfEvqzKP408JjfyGeuRAndMBH7V7w39JVfFTIRhI2JqPpmdEOaWWIQ/640?wx_fmt=png&amp;from=appmsg" data-ratio="0.4861111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000437" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGAfEvqzKP408JjfyGeuRAndMBH7V7w39JVfFTIRhI2JqPpmdEOaWWIQ/640?wx_fmt=png&amp;from=appmsg"></section><p><span leaf=""><br></span></p><hr><blockquote><h3><span leaf="">Open WebUI</span></h3></blockquote><p><span leaf="">Open WebUI 是一个自托管的 Web 界面，旨在简化与大型语言模型的交互。它通过直观的 GUI 提供对模型的管理、配置和使用支持，特别适合希望在本地或私有环境中运行 AI 的用户。Open WebUI 支持多种模型运行器（如 Ollama）和 OpenAI 兼容的 API，并内置了检索增强生成（RAG）功能，可通过文档或网页增强模型的响应质量。它的核心优势包括完全离线运行、强大的扩展性和社区驱动的开发模式。</span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGweE9gVw12gYdwQaYe4bKfthYYe7SZpruiaAZcsunpFLUqMI2a5ib4bTQ/640?wx_fmt=png&amp;from=appmsg" data-ratio="0.5601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000434" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGweE9gVw12gYdwQaYe4bKfthYYe7SZpruiaAZcsunpFLUqMI2a5ib4bTQ/640?wx_fmt=png&amp;from=appmsg"></section><p><span leaf="">这里我本地运行qwen3:14已经能正常显示，可以用这个模型进行沟通对话</span></p><p><span leaf="">Open WebUI 的设计目标是“将 AI 技术普及化”，通过降低技术门槛，让非技术用户也能轻松使用尖端 AI 模型，同时为开发者提供灵活的定制选项。</span></p><p><span leaf="">Github地址：https://github.com/open-webui/open-webui</span></p><p><span leaf="">它提供了Docker等安装方式，笔者因为之前用过，使用的是本地源码运行；故之类不展示如何运行，更多资料参考官网地址。</span></p><hr><blockquote><h3><span leaf="">Open WebUI支持MCP</span></h3></blockquote><p><span leaf="">最新版本的Open WebUI已支持MCP</span></p><p><span leaf="">Open WebUI开放了mcpo代理服务器（全称为MCP-to-OpenAPI proxy server），该协议允许你通过标准的 REST/OpenAPI 接口，直接使用基于 MCP（Model Context Protocol，模型上下文协议）实现的工具服务器——无需处理陌生或复杂的自定义协议。</span></p><p><span leaf="">如果你是终端用户或应用开发者，这意味着你可以通过熟悉的 REST 风格接口，轻松地与强大的 MCP 工具进行交互。</span></p><section nodeleaf=""><mp-common-profile data-pluginname="mpprofile" data-nickname="AI大模型观察站" data-from="0" data-headimg="http://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTwibA7LuKNnOHKXDgpHTfxaPLDO3ic0jSxUc4nPEWzLddkwp9HsI096p3SmlauWP5nGYTDtDK5cMVzg/0?wx_fmt=png" data-signature="专注于人工智能大模型的最新进展，涵盖Transformer架构、LLM训练优化、推理加速、多模态应用等核心技术领域。通过深度解析论文、开源项目和行业动态，揭示大模型技术的演进趋势，助力开发者、研究者和AI爱好者把握前沿创新。" data-id="MzkzMjkwMjk3Mw==" data-is_biz_ban="0" data-service_type="1" data-verify_status="0"></mp-common-profile></section><p><span leaf=""><br></span></p><p><span leaf="">我们按照官方示例运行如下代码：</span></p><pre><ol><li><span><span><code><span><span leaf="">uvx mcpo </span></span><span><span leaf="">--</span></span><span><span leaf="">port </span></span><span><span leaf="">8010</span></span><span></span><span><span leaf="">--</span></span><span><span leaf=""> uvx mcp</span></span><span><span leaf="">-</span></span><span><span leaf="">server</span></span><span><span leaf="">-</span></span><span><span leaf="">time </span></span><span><span leaf="">--</span></span><span><span leaf="">local</span></span><span><span leaf="">-</span></span><span><span leaf="">timezone</span></span><span><span leaf="">=</span></span><span><span leaf="">America</span></span><span><span leaf="">/</span></span><span><span leaf="">New_York</span></span></code></span></span></li><li><span><span><code><span><span leaf="">Starting</span></span><span><span leaf=""> MCP </span></span><span><span leaf="">OpenAPI</span></span><span></span><span><span leaf="">Proxy</span></span><span><span leaf=""> on </span></span><span><span leaf="">0.0</span></span><span><span leaf="">.</span></span><span><span leaf="">0.0</span></span><span><span leaf="">:</span></span><span><span leaf="">8010</span></span><span></span><span><span leaf="">with</span></span><span><span leaf=""> command</span></span><span><span leaf="">:</span></span><span><span leaf=""> uvx mcp</span></span><span><span leaf="">-</span></span><span><span leaf="">server</span></span><span><span leaf="">-</span></span><span><span leaf="">time </span></span><span><span leaf="">--</span></span><span><span leaf="">local</span></span><span><span leaf="">-</span></span><span><span leaf="">timezone</span></span><span><span leaf="">=</span></span><span><span leaf="">America</span></span><span><span leaf="">/</span></span><span><span leaf="">New_York</span></span></code></span></span></li><li><span><span><code><span><span leaf="">INFO</span></span><span><span leaf="">:</span></span><span></span><span><span leaf="">Started</span></span><span><span leaf=""> server process </span></span><span><span leaf="">[</span></span><span><span leaf="">5752</span></span><span><span leaf="">]</span></span></code></span></span></li><li><span><span><code><span><span leaf="">INFO</span></span><span><span leaf="">:</span></span><span></span><span><span leaf="">Waiting</span></span><span></span><span><span leaf="">for</span></span><span><span leaf=""> application startup</span></span><span><span leaf="">.</span></span></code></span></span></li><li><span><span><code><span><span leaf="">INFO</span></span><span><span leaf="">:</span></span><span></span><span><span leaf="">Application</span></span><span><span leaf=""> startup complete</span></span><span><span leaf="">.</span></span></code></span></span></li><li><span><span><code><span><span leaf="">INFO</span></span><span><span leaf="">:</span></span><span></span><span><span leaf="">Uvicorn</span></span><span><span leaf=""> running on http</span></span><span><span leaf="">:</span></span><span><span leaf="">//0.0.0.0:8010 (Press CTRL+C to quit)</span></span></code></span></span></li></ol></pre><p><span leaf="">这段代码的意思在本地起一个时间转换的MCP服务；可以通过Open WebUI直接调用这个服务，为用户进行服务 如果你想自定义MCP服务，请参考这里： https://docs.openwebui.com/openapi-servers/mcp/</span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGweE9gVw12gYdwQaYe4bKfthYYe7SZpruiaAZcsunpFLUqMI2a5ib4bTQ/640?wx_fmt=png&amp;from=appmsg" data-ratio="0.5601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000433" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGweE9gVw12gYdwQaYe4bKfthYYe7SZpruiaAZcsunpFLUqMI2a5ib4bTQ/640?wx_fmt=png&amp;from=appmsg"></section><p><span leaf=""><br></span></p><p><strong><span leaf="">在Open WebUI配置</span></strong><span leaf="">在web页面的设置中，选择“tools” 配置上面启动的地址+端口就行</span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGp0dmeRmusLia7ZibjZ3CwYhic98dicLNIlAhRed6Y9peTcoKwW02Y8AvEQ/640?wx_fmt=jpeg&amp;from=appmsg" data-ratio="0.6537037037037037" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="100000432" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGp0dmeRmusLia7ZibjZ3CwYhic98dicLNIlAhRed6Y9peTcoKwW02Y8AvEQ/640?wx_fmt=jpeg&amp;from=appmsg"></section><p><span leaf=""><br></span></p><p><span leaf="">在配置好大模型（选择Qwen3模型）后，在聊天框中会出现工具图标，点击该图标，显示如下：</span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGFibkaRRru73OG3XMkIa2hibHzhqOQMicOjoodj9SEdlPMGhoe5JulV3Tw/640?wx_fmt=png&amp;from=appmsg" data-ratio="0.3509259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000435" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGFibkaRRru73OG3XMkIa2hibHzhqOQMicOjoodj9SEdlPMGhoe5JulV3Tw/640?wx_fmt=png&amp;from=appmsg"></section><p><span leaf=""><br></span></p><p><span leaf="">这表明我们基于MCP的工具服务已连接成功。</span></p><p><span leaf="">在Open WebUI进行对话，如下： </span></p><section nodeleaf=""><img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGQyEIZRZgWK8obib3dUHypicRF16Pcc3vBFviab8UQYCbyLt6mibDw2VkdA/640?wx_fmt=png&amp;from=appmsg" data-ratio="0.3" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100000436" src="https://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTzhRYRCC4KBpgbPCu6AwFlGQyEIZRZgWK8obib3dUHypicRF16Pcc3vBFviab8UQYCbyLt6mibDw2VkdA/640?wx_fmt=png&amp;from=appmsg"></section><p><span leaf=""><br></span></p><p><span leaf="">从上述的对话中可以看到，大模型调用了基于MCP的gut</span><em><span leaf="">current</span></em><span leaf="">time工具，并给出了正确回复。</span></p><hr><blockquote><h3><span leaf="">总结</span></h3></blockquote><p><span leaf="">通过将 Qwen3 与 Ollama 结合进行本地部署，我成功体验了 MCP（模型-计算-平台）架构的强大潜力。这一过程不仅让我深入了解了 Qwen3 作为开源大型语言模型的卓越性能，还展示了 Ollama 在简化本地模型运行方面的便捷性和灵活性。从安装 Ollama、拉取 Qwen3 模型到优化硬件配置，整个部署流程清晰且高效，尤其适合注重数据隐私的开发者与企业用户。</span></p><p><span leaf="">在实际使用中，Qwen3 展现了出色的自然语言处理能力，无论是文本生成、问答还是代码补全，都表现得游刃有余。Ollama 的命令行界面和 API 支持让模型管理与集成变得简单，而其对 GPU 加速的优化显著提升了推理速度。通过 Open WebUI 的图形界面，我进一步体验了用户友好的交互方式，RAG 功能更是为定制化知识查询提供了便利。</span></p><p><span leaf="">然而，本地部署也面临一些挑战，例如硬件要求较高（推荐 16GB+ RAM 和独立 GPU）以及模型文件占用较大存储空间。对于初学者，配置环境和调试可能需要一定学习成本，但 Ollama 的文档和活跃社区提供了充足支持。</span></p><p><span leaf="">总的来说，Qwen3 与 Ollama 的组合为本地 AI 部署提供了高效、隐私安全的解决方案。无论是用于开发、研究还是个人探索，这一初体验让我对本地化 AI 应用的未来充满期待。未来，我计划进一步探索 Qwen3 的微调功能，并结合更复杂的 RAG 场景，为实际项目打造更强大的 AI 能力。</span></p><section nodeleaf=""><mp-common-profile data-pluginname="mpprofile" data-nickname="AI大模型观察站" data-from="0" data-headimg="http://mmbiz.qpic.cn/sz_mmbiz_png/6Ex6Atic0gTwibA7LuKNnOHKXDgpHTfxaPLDO3ic0jSxUc4nPEWzLddkwp9HsI096p3SmlauWP5nGYTDtDK5cMVzg/0?wx_fmt=png" data-signature="专注于人工智能大模型的最新进展，涵盖Transformer架构、LLM训练优化、推理加速、多模态应用等核心技术领域。通过深度解析论文、开源项目和行业动态，揭示大模型技术的演进趋势，助力开发者、研究者和AI爱好者把握前沿创新。" data-id="MzkzMjkwMjk3Mw==" data-is_biz_ban="0" data-service_type="1" data-verify_status="0"></mp-common-profile></section><p><span leaf=""><br></span></p><p><span leaf="">如果你有更多玩法，请私信，我们一起玩耍🎉</span></p><p><span leaf="">每天进步一点点🚀🚀🚀🚀🚀</span></p><section><span leaf=""><br></span></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/RpnGHAdrKCa_j2TqeFztgw",target="_blank" rel="noopener noreferrer">原文链接</a>
